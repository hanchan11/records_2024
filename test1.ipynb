{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 问题：\n",
    "#    1.分位数计算 --- 线性方法 （不同的方法会带来不同的结果）  但R语言应该采用的是linear方法 \n",
    "#    2.分位数的值 --- 值的变化会对结果带来明显影响  ---  有一定可能R语言里的分位数和python中的分位数存在些许差异   但感觉可能性不大\n",
    "#    3.R语言里的gev和lmoment  ---  感觉在python中的表示存在着一定的问题  想要着重考虑\n",
    "# 解决方案：\n",
    "#    1/2 --- 对应分位数选择和选取的方法\n",
    "#    3 --- 对应lmoments3库\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import genextreme\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import lmoments3 as lm3\n",
    "import lmoments as lm\n",
    "\n",
    "def find_mf_func(stations_tab, amax_tab, start_year):\n",
    "    megafloods_df = pd.DataFrame(columns=['id', 'LAT', 'LON', 'area', 'Q_MF', 'year_MF', 'date_MF'])\n",
    "  \n",
    "    nn = 0  # count number of outliers\n",
    "    for i in range(len(stations_tab)):\n",
    "        station_id = stations_tab['id'][i]\n",
    "        dummy = amax_tab[amax_tab['id'] == station_id].dropna(subset=['q'])\n",
    "        # dummy = amax_tab[amax_tab['id'] == station_id].dropna(subset=['q'])\n",
    "        # dummy = amax_tab[amax_tab['id'] == station_id]\n",
    "        \n",
    "        if len(dummy) < 10:\n",
    "            continue  # exclude short series\n",
    "        \n",
    "        # megafloods criteria:\n",
    "        \n",
    "        # dummy_quant = dummy['q'].quantile([0.25, 0.75])\n",
    "        # dummy_upper = dummy_quant.loc[0.75] + 3 * (dummy_quant.loc[0.75] - dummy_quant.loc[0.25])\n",
    "        # index_outlier = dummy[dummy['q'] >= dummy_upper].index\n",
    "\n",
    "        # 1. outlier \n",
    "        dummy_sorted = sorted(dummy[\"q\"])\n",
    "        n = len(dummy_sorted)\n",
    "        q1_index = int(0.25 * n) \n",
    "        q3_index = int(0.75 * n) \n",
    "        q1 = dummy_sorted[q1_index]\n",
    "        q3 = dummy_sorted[q3_index]\n",
    "        dummy_upper = q3 + 3 * (q3 - q1)\n",
    "        # # 分位数计算，很怪  还行\n",
    "\n",
    "        index_outlier = dummy[dummy['q'] >= dummy_upper].index\n",
    "        if len(index_outlier) == 0:\n",
    "            continue\n",
    "        \n",
    "        index_MF = []\n",
    "        for tt in index_outlier:\n",
    "            ty = dummy['year'][tt]\n",
    "            # nyears_before = len(dummy[dummy['year'] < ty])   # 可能存在的问题 在下一行改了之后并没有明显影响\n",
    "            nyears_before = len(dummy[dummy['year'] < ty]['q'])\n",
    "            \n",
    "            # 3. after start.year and with at least 20 years of data before event\n",
    "            if nyears_before < 20 or ty < start_year:\n",
    "                continue\n",
    "            \n",
    "        # # index_MF = []  # 存储异常事件的索引\n",
    "\n",
    "        # # for tt in range(1, len(dummy)):\n",
    "        #     qmax_old = dummy[dummy['year'] <= ty] ['q'] # 根据年份选择流量值的子集\n",
    "\n",
    "        # # # 计算L-moments\n",
    "        # # lmoments = lm3.lmom_ratios(qmax_old.values)\n",
    "\n",
    "        # # 使用L-moments进行GEV分布拟合\n",
    "        #     params = lm.pelgev(qmax_old)\n",
    "        \n",
    "\n",
    "        # # 使用pextRemes计算概率\n",
    "        #     neP = genextreme.cdf(qmax_old,*params)\n",
    "        #     RP = 1 / (1 - neP)\n",
    "        #     RP[np.isinf(RP)] = 10**5\n",
    "        #     rRP = RP[-1] / max(RP[:-1])\n",
    "\n",
    "        #     if rRP > 3:\n",
    "        #         index_MF.append(tt)\n",
    "\n",
    "        # index_outlier = index_MF[1:]\n",
    "\n",
    "            # 2. surprising (Tl/Tsl > 3) record-breaking event\n",
    "            qmax_old = dummy[dummy['year'] <= ty]['q']    # 同上的问题\n",
    "            # 计算概率分布函数\n",
    "           \n",
    "\n",
    "            local_gev = genextreme.fit(qmax_old)\n",
    "            neP = 1-genextreme.pdf(qmax_old, *local_gev)   # 方法计算对吗  这是真的大问题\n",
    "           \n",
    "            RP = 1 / (1 - neP)\n",
    "            RP[RP == float('inf')] = 10 ** 5\n",
    "            rRP = RP[-1] / max(RP[:-1])\n",
    "            if rRP > 3:\n",
    "                index_MF.append(tt)\n",
    "                \n",
    "        index_outlier = index_MF\n",
    "        if len(index_MF) > 0:\n",
    "            for oo in index_MF:\n",
    "                nn += 1\n",
    "                try:\n",
    "                    megafloods_df.loc[nn] = [stations_tab['id'][i], stations_tab['lat'][i], stations_tab['lon'][i],\n",
    "                                         float(stations_tab['area'][i]), dummy['q'][oo], dummy['year'][oo],\n",
    "                                         pd.to_datetime(\n",
    "                                             f\"{int(dummy['year'][oo])}-{int(dummy['month'][oo])}-{int(dummy['day'][oo])}\")]\n",
    "                except ValueError:\n",
    "                    megafloods_df.loc[nn] = [stations_tab['id'][i], stations_tab['lat'][i], stations_tab['lon'][i],\n",
    "                                         float(stations_tab['area'][i]), dummy['q'][oo], dummy['year'][oo],-1]\n",
    "    \n",
    "    return megafloods_df\n",
    "\n",
    "stations = pd.read_csv(\"./gauges.csv\")\n",
    "flood_data = pd.read_csv(\"./floods.csv\")\n",
    "megafloods_df = find_mf_func(stations,flood_data,2000)\n",
    "# print(find_mf_func(stations,flood_data,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import genextreme\n",
    "\n",
    "# 2. surprising (Tl/Tsl>3) record-breaking event\n",
    "qmax_old = dummy['q'][dummy['year'] <= ty].tolist()\n",
    "\n",
    "# 进行GEV拟合\n",
    "c, loc, scale = genextreme.fit(qmax_old)\n",
    "\n",
    "# 计算概率分布函数\n",
    "neP = genextreme.sf(qmax_old, c, loc=loc, scale=scale)\n",
    "\n",
    "# 计算RP和rRP\n",
    "RP = 1 / (1 - neP)\n",
    "RP[np.isinf(RP)] = 10 ** 5\n",
    "rRP = RP[-1] / max(RP[:-1])\n",
    "\n",
    "if rRP > 3:\n",
    "    index_MF.append(tt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解决方案3\n",
    "import numpy as np\n",
    "import lmoments as lm\n",
    "import lmoments3 as lm3\n",
    "# 假设 dummy 是包含年份和流量数据的数据框\n",
    "# 假设 ty 是阈值年份\n",
    "# 在这里，你需要根据你的数据情况对 dummy 和 ty 进行适当的赋值\n",
    "\n",
    "index_MF = []  # 存储异常事件的索引\n",
    "\n",
    "for tt in range(1, len(dummy)):\n",
    "    qmax_old = dummy.loc[dummy['year'] <= ty, 'q']  # 根据年份选择流量值的子集\n",
    "\n",
    "    # 计算L-moments\n",
    "    lmoments = lm.lmom_ratios(qmax_old.values, nmom=3)\n",
    "\n",
    "    # 使用L-moments进行GEV分布拟合\n",
    "    params = lm.pelgev(lmoments)\n",
    "\n",
    "    # 使用pextRemes计算概率\n",
    "    neP = lm3.pextRemes(params, q=qmax_old.values)\n",
    "    RP = 1 / (1 - neP)\n",
    "    RP[np.isinf(RP)] = 10**5\n",
    "    rRP = RP[-1] / max(RP[:-1])\n",
    "\n",
    "    if rRP > 3:\n",
    "        index_MF.append(tt)\n",
    "\n",
    "index_outlier = index_MF[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二部分\n",
    "# 1.平均值和最大值\n",
    "import pandas as pd\n",
    "\n",
    "stations = pd.read_csv(\"./gauges.csv\")\n",
    "flood_data = pd.read_csv(\"./floods.csv\")\n",
    "\n",
    "\n",
    "# 计算平均值和最大值\n",
    "mean_q = flood_data.iloc[:, 1].groupby(flood_data['id']).mean().round(5)\n",
    "max_q = flood_data.iloc[:, 1].groupby(flood_data['id']).max()\n",
    "\n",
    "# 将结果赋给 stations 数据框\n",
    "stations['mean_q'] = stations['id'].map(mean_q)\n",
    "stations['max_q'] = stations['id'].map(max_q)\n",
    "\n",
    "# print(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.stats.mstats' has no attribute 'rq'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m stations \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./gauges.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m flood_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./floods.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43menv_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflood_data\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m, in \u001b[0;36menv_func\u001b[1;34m(stations_tab, amax_tab)\u001b[0m\n\u001b[0;32m     22\u001b[0m     dummy_q[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlog10\u001b[39m\u001b[38;5;124m'\u001b[39m][ind_stat] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog10(dummy_stations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m][i])  \n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 进行分位数回归  \u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Quantile regression z=0.5:  \u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m dummy_rq \u001b[38;5;241m=\u001b[39m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrq\u001b[49m(formula\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqlog10 ~ Alog10\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mdummy_q, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbct\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[0;32m     27\u001b[0m ENV_tab\u001b[38;5;241m.\u001b[39mloc[rr, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslope_qr_median\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dummy_rq\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;241m1\u001b[39m]  \n\u001b[0;32m     28\u001b[0m ENV_tab\u001b[38;5;241m.\u001b[39mloc[rr, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintercept_qr_median\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dummy_rq\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;241m0\u001b[39m]  \n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scipy.stats.mstats' has no attribute 'rq'"
     ]
    }
   ],
   "source": [
    "# 第二部分 try1 \n",
    "# 看上去和下面的差不多啊，问题在哪里呢？（统计库的问题）\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from scipy import stats  \n",
    "  \n",
    "# print(stations)\n",
    "def env_func(stations_tab, amax_tab):  \n",
    "    # 初始化数据框  \n",
    "    ENV_tab = pd.DataFrame(columns=['slope_qr_median', 'intercept_qr_median', 'slope_qr_env', 'intercept_qr_env', 'intercept_env', 'n_stat_years'])\n",
    "      \n",
    "    # 循环处理数据  \n",
    "    for rr in range(5):  \n",
    "        # 提取特定区域的数据  \n",
    "        dummy_stations = stations_tab[stations_tab['regions1'] == rr]  \n",
    "        dummy_q = amax_tab[amax_tab['id'].isin(dummy_stations['id'])]  \n",
    "        dummy_q = dummy_q[dummy_q['q'] > 0]  \n",
    "          \n",
    "        # 计算log转换  \n",
    "        dummy_q['qlog10'] = np.log10(dummy_q['q'])  \n",
    "        dummy_q['Alog10'] = np.nan  \n",
    "        for i in range(len(dummy_stations)):  \n",
    "            ind_stat = np.where(dummy_q['id'] == dummy_stations['id'][i])[0][0]  \n",
    "            dummy_q['Alog10'][ind_stat] = np.log10(dummy_stations['area'][i])  \n",
    "          \n",
    "        # 进行分位数回归  \n",
    "        # Quantile regression z=0.5:  \n",
    "        dummy_rq = stats.mstats.rq(formula='qlog10 ~ Alog10', data=dummy_q, tau=0.5, method='bct')  \n",
    "        ENV_tab.loc[rr, 'slope_qr_median'] = dummy_rq.params[1]  \n",
    "        ENV_tab.loc[rr, 'intercept_qr_median'] = dummy_rq.params[0]  \n",
    "        # Quantile regression z=0.999:  \n",
    "        dummy_rq = stats.mstats.rq(formula='qlog10 ~ Alog10', data=dummy_q, tau=0.999, method='bct')  \n",
    "        ENV_tab.loc[rr, 'slope_qr_env'] = dummy_rq.params[1]  \n",
    "        ENV_tab.loc[rr, 'intercept_qr_env'] = dummy_rq.params[0]  \n",
    "        ENV_tab.loc[rr, 'intercept_env'] = np.max(dummy_q['Alog10'])  \n",
    "        ENV_tab.loc[rr, 'n_stat_years'] = len(dummy_q)  \n",
    "    return ENV_tab\n",
    "\n",
    "# ENV_reg = env_func(stations, flood_data)\n",
    "stations = pd.read_csv(\"./gauges.csv\")\n",
    "flood_data = pd.read_csv(\"./floods.csv\")\n",
    "print(env_func(stations, flood_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'conditional_logit',\n",
       " 'conditional_mnlogit',\n",
       " 'conditional_poisson',\n",
       " 'gee',\n",
       " 'glm',\n",
       " 'glmgam',\n",
       " 'gls',\n",
       " 'glsar',\n",
       " 'logit',\n",
       " 'mixedlm',\n",
       " 'mnlogit',\n",
       " 'negativebinomial',\n",
       " 'nominal_gee',\n",
       " 'ols',\n",
       " 'ordinal_gee',\n",
       " 'phreg',\n",
       " 'poisson',\n",
       " 'probit',\n",
       " 'quantreg',\n",
       " 'rlm',\n",
       " 'wls']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "dir(smf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二部分1：五个区域的包络曲线\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def env_func(stations_tab, amax_tab):\n",
    "    # Initialize an empty DataFrame for the results\n",
    "    columns = ['slope_qr_median', 'intercept_qr_median', 'slope_qr_env', 'intercept_qr_env', 'intercept_env', 'n_stat_years']\n",
    "    ENV_tab = pd.DataFrame(columns=columns, index=range(1, 6))\n",
    "    \n",
    "    for rr in range(1, 6):\n",
    "        # print(rr)     打印这个东西干嘛?\n",
    "\n",
    "        # Filter stations and amax data by region\n",
    "        dummy_stations = stations_tab[stations_tab['regions1'] == rr]\n",
    "        dummy_q = amax_tab[amax_tab['id'].isin(dummy_stations['id'])]\n",
    "        dummy_q = dummy_q[dummy_q['q'] > 0]\n",
    "        dummy_q['qlog10'] = np.log10(dummy_q['q'])\n",
    "        dummy_q['Alog10'] = np.nan\n",
    "\n",
    "        # Assign area log10 values\n",
    "        for i in dummy_stations['id']:\n",
    "            dummy_q.loc[dummy_q['id'] == i, 'Alog10'] = np.log10(dummy_stations.loc[dummy_stations['id'] == i, 'area'].values[0])\n",
    "\n",
    "        # Quantile regression for tau=0.5\n",
    "        model_median = smf.quantreg('qlog10 ~ Alog10', dummy_q).fit(q=0.5)\n",
    "        ENV_tab.at[rr, 'slope_qr_median'] = model_median.params['Alog10']\n",
    "        ENV_tab.at[rr, 'intercept_qr_median'] = model_median.params['Intercept']\n",
    "\n",
    "        # Quantile regression for tau=0.999\n",
    "        model_env = smf.quantreg('qlog10 ~ Alog10', dummy_q).fit(q=0.999)\n",
    "        ENV_tab.at[rr, 'slope_qr_env'] = model_env.params['Alog10']\n",
    "        ENV_tab.at[rr, 'intercept_qr_env'] = model_env.params['Intercept']\n",
    "\n",
    "        # Calculate intercept of envelope\n",
    "        intercept = -ENV_tab.at[rr, 'slope_qr_env'] * np.log10(dummy_stations['area']) + np.log10(dummy_stations['max_q'])\n",
    "        ENV_tab.at[rr, 'intercept_env'] = intercept.max()\n",
    "\n",
    "        # Number of station years\n",
    "        ENV_tab.at[rr, 'n_stat_years'] = len(dummy_q)\n",
    "\n",
    "    return ENV_tab\n",
    "# stations = pd.read_csv(\"./gauges.csv\")\n",
    "# flood_data = pd.read_csv(\"./floods.csv\")\n",
    "# print(env_func(stations, flood_data))\n",
    "ENV_reg = env_func(stations, flood_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Slope_Envelope Intercept_Envelope Intercept_Envelope_1000km2  \\\n",
      "1      -0.074694           0.359792                   1.366811   \n",
      "2      -0.382777           1.485215                   2.172128   \n",
      "3      -0.566036           2.419434                   5.264131   \n",
      "4      -0.270014           1.487376                   4.756991   \n",
      "5      -0.337627           1.486842                   2.978257   \n",
      "\n",
      "  Slope_median_regression Intercept_median_regression  \n",
      "1               -0.091551                   -0.879714  \n",
      "2               -0.217651                   -0.546294  \n",
      "3               -0.284169                   -0.021829  \n",
      "4               -0.199583                   -0.177523  \n",
      "5               -0.146376                   -0.647347  \n"
     ]
    }
   ],
   "source": [
    "# 第二部分2：EDT1 and EDT2\n",
    "# 未订正\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log10\n",
    "\n",
    "# EDT 1\n",
    "def ext_data_tab1_func(envelope_tab):\n",
    "    ExtDataTable1 = pd.DataFrame(np.nan, index=envelope_tab.index, columns=[\"Slope_Envelope\",\"Intercept_Envelope\",\"Intercept_Envelope_1000km2\",\"Slope_median_regression\",\"Intercept_median_regression\"])\n",
    "    ExtDataTable1[\"Slope_Envelope\"] = envelope_tab[\"slope_qr_env\"]\n",
    "    ExtDataTable1[\"Intercept_Envelope\"] = envelope_tab[\"intercept_env\"]\n",
    "    ExtDataTable1[\"Intercept_Envelope_1000km2\"] = 10 ** (envelope_tab[\"intercept_env\"] + envelope_tab[\"slope_qr_env\"] * log10(1000))\n",
    "    ExtDataTable1[\"Slope_median_regression\"] = envelope_tab[\"slope_qr_median\"]\n",
    "    ExtDataTable1[\"Intercept_median_regression\"] = envelope_tab[\"intercept_qr_median\"]\n",
    "    ExtDataTable1 = ExtDataTable1.round(2)\n",
    "    return ExtDataTable1\n",
    "\n",
    "ExtDataTable1 = ext_data_tab1_func(ENV_reg)\n",
    "print(ExtDataTable1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number_of_gauges  Avg_Ratio_max_mean_Q  Number_of_targets  \\\n",
      "1             671.0                  2.09                7.0   \n",
      "2            3660.0                  3.25              262.0   \n",
      "3             938.0                  3.50               21.0   \n",
      "4            1240.0                  2.83               80.0   \n",
      "5            1514.0                  2.36              121.0   \n",
      "\n",
      "   Percentage_of_targets  Avg_Ratio_max_mean_Q_targets  \n",
      "1                    1.0                          2.92  \n",
      "2                    7.2                          5.40  \n",
      "3                    2.2                          5.39  \n",
      "4                    6.5                          4.03  \n",
      "5                    8.0                          3.44  \n"
     ]
    }
   ],
   "source": [
    "# EDT 2\n",
    "ExtDataTable2 = pd.DataFrame(np.nan, index=range(1, 6), columns=[\"Number_of_gauges\",\"Avg_Ratio_max_mean_Q\",\"Number_of_targets\", \"Percentage_of_targets\",\"Avg_Ratio_max_mean_Q_targets\"])\n",
    "for rr in range(1, 6):\n",
    "    dummy_stations = stations[stations[\"regions1\"] == rr]\n",
    "    ExtDataTable2.loc[rr, \"Number_of_gauges\"] = dummy_stations.shape[0]\n",
    "    ExtDataTable2.loc[rr, \"Avg_Ratio_max_mean_Q\"] = (dummy_stations[\"max_q\"] / dummy_stations[\"mean_q\"]).mean().round(2)\n",
    "    dummy_mf = dummy_stations[dummy_stations[\"id\"].isin(megafloods_df[\"id\"])]\n",
    "    ExtDataTable2.loc[rr, \"Number_of_targets\"] = dummy_mf.shape[0]\n",
    "    ExtDataTable2.loc[rr, \"Percentage_of_targets\"] = (ExtDataTable2.loc[rr, \"Number_of_targets\"] * 100 / ExtDataTable2.loc[rr, \"Number_of_gauges\"]).round(1)\n",
    "    ExtDataTable2.loc[rr, \"Avg_Ratio_max_mean_Q_targets\"] = (dummy_mf[\"max_q\"] / dummy_mf[\"mean_q\"]).mean().round(2)\n",
    "\n",
    "del dummy_mf, dummy_stations\n",
    "\n",
    "print(ExtDataTable2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstations\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stations' is not defined"
     ]
    }
   ],
   "source": [
    "print(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type numpy.float64 which has no callable log10 method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'log10'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m         EU_MHQ_CV_list[target_year] \u001b[38;5;241m=\u001b[39m EU_MHQ_CV\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMHQ_log\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea_log\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EU_MHQ_CV_list\n\u001b[1;32m---> 37\u001b[0m EU_MHQ_CV_list \u001b[38;5;241m=\u001b[39m \u001b[43mmhq_cv_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflood_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(EU_MHQ_CV_list)\n",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m, in \u001b[0;36mmhq_cv_func\u001b[1;34m(stations_df, amax_df, target_years)\u001b[0m\n\u001b[0;32m     28\u001b[0m     EU_MHQ_CV\u001b[38;5;241m.\u001b[39mat[index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCV\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dummy_Q\u001b[38;5;241m.\u001b[39mstd() \u001b[38;5;241m/\u001b[39m EU_MHQ_CV\u001b[38;5;241m.\u001b[39mat[index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMHQ\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Normalize MHQ to 100km2\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m EU_MHQ_CV[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMHQ_log\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEU_MHQ_CV\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMHQ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m EU_MHQ_CV[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea_log\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog10(stations_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     32\u001b[0m area_regr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpolyfit(EU_MHQ_CV[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea_log\u001b[39m\u001b[38;5;124m'\u001b[39m], EU_MHQ_CV[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMHQ_log\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\generic.py:2102\u001b[0m, in \u001b[0;36mNDFrame.__array_ufunc__\u001b[1;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array_ufunc__\u001b[39m(\n\u001b[0;32m   2100\u001b[0m     \u001b[38;5;28mself\u001b[39m, ufunc: np\u001b[38;5;241m.\u001b[39mufunc, method: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39minputs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m   2101\u001b[0m ):\n\u001b[1;32m-> 2102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arraylike\u001b[38;5;241m.\u001b[39marray_ufunc(\u001b[38;5;28mself\u001b[39m, ufunc, method, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\arraylike.py:396\u001b[0m, in \u001b[0;36marray_ufunc\u001b[1;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;66;03m# ufunc(series, ...)\u001b[39;00m\n\u001b[0;32m    395\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(extract_array(x, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[1;32m--> 396\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ufunc, method)(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;66;03m# ufunc(dataframe)\u001b[39;00m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs:\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;66;03m# for np.<ufunc>(..) calls\u001b[39;00m\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;66;03m# kwargs cannot necessarily be handled block-by-block, so only\u001b[39;00m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;66;03m# take this path if there are no kwargs\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type numpy.float64 which has no callable log10 method"
     ]
    }
   ],
   "source": [
    "# 第三部分:try1   方法不对\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def mhq_cv_func(stations_df, amax_df, target_years):\n",
    "    EU_MHQ_CV_list = {}\n",
    "    for target_year in target_years:\n",
    "        print(target_year)\n",
    "        EU_MHQ_CV = pd.DataFrame(columns=[\"MHQ\", \"CV\", \"MHQ_100\"])\n",
    "        EU_MHQ_CV = pd.concat([stations_df[['id', 'lat', 'lon', 'area']], EU_MHQ_CV], axis=1)\n",
    "        for index, station in EU_MHQ_CV.iterrows():\n",
    "            dummy_df = amax_df[(amax_df['id'] == station['id']) & (amax_df['year'] < target_year)]\n",
    "            dummy_df = dummy_df.dropna(subset=['q'])\n",
    "            if len(dummy_df) < 10: \n",
    "                continue\n",
    "            dummy_Q = dummy_df['q'].values\n",
    "            dummy_Y = dummy_df['year'].values\n",
    "            # Exclude outliers\n",
    "            Q1 = np.quantile(dummy_Q, 0.25)\n",
    "            Q3 = np.quantile(dummy_Q, 0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            upper_bound = Q3 + 3 * IQR\n",
    "            filtered_indices = dummy_Q <= upper_bound\n",
    "            dummy_Q = dummy_Q[filtered_indices]\n",
    "            dummy_Y = dummy_Y[filtered_indices]\n",
    "            EU_MHQ_CV.at[index, 'MHQ'] = dummy_Q.mean()\n",
    "            EU_MHQ_CV.at[index, 'CV'] = dummy_Q.std() / EU_MHQ_CV.at[index, 'MHQ']\n",
    "        # Normalize MHQ to 100km2\n",
    "        EU_MHQ_CV['MHQ_log'] = np.log10(EU_MHQ_CV['MHQ'])\n",
    "        EU_MHQ_CV['area_log'] = np.log10(stations_df['area'])\n",
    "        area_regr = np.polyfit(EU_MHQ_CV['area_log'], EU_MHQ_CV['MHQ_log'], 1)\n",
    "        EU_MHQ_CV['MHQ_100'] = 10 ** (EU_MHQ_CV['MHQ_log'] - area_regr[0] * EU_MHQ_CV['area_log'] + area_regr[0] * np.log10(100))\n",
    "        EU_MHQ_CV_list[target_year] = EU_MHQ_CV.drop(columns=['MHQ_log', 'area_log'])\n",
    "    return EU_MHQ_CV_list\n",
    "\n",
    "EU_MHQ_CV_list = mhq_cv_func(stations, flood_data, range(2000, 2023))\n",
    "print(EU_MHQ_CV_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m         EU_MHQ_CV_list[target_year] \u001b[38;5;241m=\u001b[39m EU_MHQ_CV\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMHQ_log\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea_log\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EU_MHQ_CV_list\n\u001b[1;32m---> 42\u001b[0m EU_MHQ_CV_list \u001b[38;5;241m=\u001b[39m mhq_cv_func(\u001b[43mstations\u001b[49m, flood_data, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m2023\u001b[39m))\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(EU_MHQ_CV_list)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stations' is not defined"
     ]
    }
   ],
   "source": [
    "# 第三部分:try2 似乎流程正确，但总感觉差点什么\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def mhq_cv_func(stations_df, amax_df, target_years):\n",
    "    EU_MHQ_CV_list = {}\n",
    "    for target_year in target_years:\n",
    "        # print(target_year)\n",
    "        EU_MHQ_CV = pd.DataFrame(columns=[\"MHQ\", \"CV\", \"MHQ_100\"])\n",
    "        EU_MHQ_CV = pd.concat([stations_df[['id', 'lat', 'lon', 'area']], EU_MHQ_CV], axis=1)\n",
    "        for index, station in EU_MHQ_CV.iterrows():\n",
    "            dummy_df = amax_df[(amax_df['id'] == station['id']) & (amax_df['year'] < target_year)]\n",
    "            dummy_df = dummy_df.dropna(subset=['q'])\n",
    "            print(f'Year: {target_year}, Station ID: {station[\"id\"]}, Dummy DF Length: {len(dummy_df)}')\n",
    "            if len(dummy_df) < 10: \n",
    "                continue\n",
    "            dummy_Q = dummy_df['q'].values\n",
    "            dummy_Y = dummy_df['year'].values\n",
    "            # Exclude outliers\n",
    "            Q1 = np.quantile(dummy_Q, 0.25)\n",
    "            Q3 = np.quantile(dummy_Q, 0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            upper_bound = Q3 + 3 * IQR\n",
    "            filtered_indices = dummy_Q <= upper_bound\n",
    "            dummy_Q = dummy_Q[filtered_indices]\n",
    "            dummy_Y = dummy_Y[filtered_indices]\n",
    "            EU_MHQ_CV.at[index, 'MHQ'] = np.mean(dummy_Q)\n",
    "            EU_MHQ_CV.at[index, 'CV'] = np.std(dummy_Q) / EU_MHQ_CV.at[index, 'MHQ']\n",
    "        # Normalize MHQ to 100km2\n",
    "        # Normalize MHQ to 100km2\n",
    "        EU_MHQ_CV['MHQ'] = EU_MHQ_CV['MHQ'].replace(0, np.nan)  # Replace 0 with NaN to avoid log(0) errors\n",
    "        EU_MHQ_CV['MHQ_log'] = np.log10(EU_MHQ_CV['MHQ'].astype(float))  # Corrected\n",
    "        stations_df = stations_df.dropna(subset=['area'])\n",
    "        EU_MHQ_CV['area_log'] = np.log10(stations_df['area'].astype(float))  # Corrected\n",
    "        area_regr = np.polyfit(EU_MHQ_CV['area_log'], EU_MHQ_CV['MHQ_log'], 1)\n",
    "        EU_MHQ_CV['MHQ_100'] = 10 ** (EU_MHQ_CV['MHQ_log'] - area_regr[0] * EU_MHQ_CV['area_log'] + area_regr[0] * np.log10(100))\n",
    "        EU_MHQ_CV_list[target_year] = EU_MHQ_CV.drop(columns=['MHQ_log', 'area_log'])\n",
    "    return EU_MHQ_CV_list\n",
    "\n",
    "\n",
    "EU_MHQ_CV_list = mhq_cv_func(stations, flood_data, range(2000, 2023))\n",
    "print(EU_MHQ_CV_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 36\u001b[0m\n\u001b[0;32m     32\u001b[0m         ENV_tab\u001b[38;5;241m.\u001b[39mloc[rr, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_stat_years\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dummy_q)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ENV_tab\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43menv_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflood_data\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36menv_func\u001b[1;34m(stations_tab, amax_tab)\u001b[0m\n\u001b[0;32m     17\u001b[0m     dummy_q\u001b[38;5;241m.\u001b[39mat[ind_stat, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlog10\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog10(dummy_stations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m][i])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Quantile regression z=0.5\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQuantReg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_q\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqlog10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_constant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_q\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAlog10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m res \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(q\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     22\u001b[0m ENV_tab\u001b[38;5;241m.\u001b[39mloc[rr, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslope_qr_median\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlog10\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\statsmodels\\regression\\quantile_regression.py:79\u001b[0m, in \u001b[0;36mQuantReg.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_kwargs(kwargs)\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28msuper\u001b[39m(QuantReg, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:202\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28msuper\u001b[39m(RegressionModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog: Float64Array \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attr\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwendog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\statsmodels\\base\\model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\statsmodels\\base\\model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m     96\u001b[0m                               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\statsmodels\\base\\model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 135\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data(endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\statsmodels\\base\\data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    672\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[0;32m    674\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m klass(endog, exog\u001b[38;5;241m=\u001b[39mexog, missing\u001b[38;5;241m=\u001b[39mmissing, hasconst\u001b[38;5;241m=\u001b[39mhasconst,\n\u001b[0;32m    676\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\statsmodels\\base\\data.py:88\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_constant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity()\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\statsmodels\\base\\data.py:132\u001b[0m, in \u001b[0;36mModelData._handle_constant\u001b[1;34m(self, hasconst)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# detect where the constant is\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     check_implicit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m     exog_max \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(exog_max)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MissingDataError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexog contains inf or nans\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "# 第二部分：try2 \n",
    "# 还是数据统计计算的问题，以及矩阵大小似乎有点不太对\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# print(stations)\n",
    "def env_func(stations_tab, amax_tab):\n",
    "    ENV_tab = pd.DataFrame(columns=['slope_qr_median', 'intercept_qr_median', 'slope_qr_env', 'intercept_qr_env', 'intercept_env', 'n_stat_years'])\n",
    "\n",
    "    for rr in range(5):\n",
    "        dummy_stations = stations_tab[stations_tab['regions1'] == rr]\n",
    "        dummy_q = amax_tab[amax_tab['id'].isin(dummy_stations['id']) & (amax_tab['q'] > 0)]\n",
    "\n",
    "        dummy_q['qlog10'] = np.log10(dummy_q['q'] > 0)\n",
    "        dummy_q['Alog10'] = np.nan\n",
    "        for i in range(len(dummy_stations)):\n",
    "            ind_stat = np.where(dummy_q['id'] == dummy_stations['id'][i])[0][0]\n",
    "            dummy_q.at[ind_stat, 'Alog10'] = np.log10(dummy_stations['area'][i])\n",
    "\n",
    "        # Quantile regression z=0.5\n",
    "        model = sm.QuantReg(dummy_q['qlog10'], sm.add_constant(dummy_q['Alog10']))\n",
    "        res = model.fit(q=0.5)\n",
    "        ENV_tab.loc[rr, 'slope_qr_median'] = res.params['Alog10']\n",
    "        ENV_tab.loc[rr, 'intercept_qr_median'] = res.params['const']\n",
    "\n",
    "        # Quantile regression z=0.999\n",
    "        model = sm.QuantReg(dummy_q['qlog10'], sm.add_constant(dummy_q['Alog10']))\n",
    "        res = model.fit(q=0.999)\n",
    "        ENV_tab.loc[rr, 'slope_qr_env'] = res.params['Alog10']\n",
    "        ENV_tab.loc[rr, 'intercept_qr_env'] = res.params['const']\n",
    "\n",
    "        ENV_tab.loc[rr, 'intercept_env'] = np.max(dummy_q['Alog10'])\n",
    "        ENV_tab.loc[rr, 'n_stat_years'] = len(dummy_q)\n",
    "\n",
    "    return ENV_tab\n",
    "\n",
    "print(env_func(stations, flood_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\xiaotian\\Rcode_rewrite\\arcpy_code\\floods.csv\n",
      "Absolute path: E:\\xiaotian\\Rcode_rewrite\\arcpy_code\\floods.csv\n",
      "File name: floods.csv\n",
      "Path root: \\\n",
      "Parent directory: E:\\xiaotian\\Rcode_rewrite\\arcpy_code\n",
      "File extension: .csv\n",
      "Is it absolute: True\n"
     ]
    }
   ],
   "source": [
    "# 额外1：pathlib 库的路径查询？\n",
    "from pathlib import Path\n",
    "readme = Path(\"floods.csv\").resolve()\n",
    "\n",
    "print(readme) # resolve()和absolute()打印结果一致\n",
    "print(f\"Absolute path: {readme.absolute()}\")\n",
    "# Absolute path: /home/martin/some/path/README.md\n",
    "print(f\"File name: {readme.name}\")\n",
    "# File name: README.md\n",
    "print(f\"Path root: {readme.root}\")\n",
    "# Path root: /\n",
    "print(f\"Parent directory: {readme.parent}\")\n",
    "# Parent directory: /home/martin/some/path\n",
    "print(f\"File extension: {readme.suffix}\")\n",
    "# File extension: .md\n",
    "print(f\"Is it absolute: {readme.is_absolute()}\")\n",
    "# Is it absolute: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\etc\\cron.d\\anacron\n",
      "Exists? - False\n"
     ]
    }
   ],
   "source": [
    "# 额外2：连接路径\n",
    "from pathlib import Path\n",
    "# Operators:\n",
    "etc = Path('/etc')\n",
    " \n",
    " \n",
    "joined = etc / \"cron.d\" / \"anacron\"\n",
    "print(joined)\n",
    "print(f\"Exists? - {joined.exists()}\")\n",
    "# Exists? - True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\xiaotian\\01.csv\n",
      "E:\\xiaotian\\01.csv\n"
     ]
    }
   ],
   "source": [
    "# 额外3：查询路径+连接路径的正确玩法\n",
    "from pathlib import Path\n",
    "\n",
    "input_str1 =Path( r'E:\\xiaotian') # 输入路径前缀\n",
    "input_str2 = '01.csv' # 输入路径后缀\n",
    "joined = input_str1/input_str2 # 完整路径\n",
    "print(joined)\n",
    "readme = joined.resolve() # 查询完整路径对象\n",
    "print(readme)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statistical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
