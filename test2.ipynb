{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 问题：\n",
    "#    1.分位数计算 --- 线性方法 （不同的方法会带来不同的结果）  但R语言应该采用的是linear方法 \n",
    "#    2.分位数的值 --- 值的变化会对结果带来明显影响  ---  有一定可能R语言里的分位数和python中的分位数存在些许差异   但感觉可能性不大\n",
    "#    3.R语言里的gev和lmoment  ---  感觉在python中的表示存在着一定的问题  想要着重考虑\n",
    "# 解决方案：\n",
    "#    1/2 --- 对应分位数选择和选取的方法\n",
    "#    3 --- 对应lmoments3库\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import genextreme\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import lmoments3 as lm3\n",
    "import lmoments as lm\n",
    "\n",
    "def find_mf_func(stations_tab, amax_tab, start_year):\n",
    "    megafloods_df = pd.DataFrame(columns=['id', 'LAT', 'LON', 'area', 'Q_MF', 'year_MF', 'date_MF'])\n",
    "  \n",
    "    nn = 0  # count number of outliers\n",
    "    for i in range(len(stations_tab)):\n",
    "        station_id = stations_tab['id'][i]\n",
    "        dummy = amax_tab[amax_tab['id'] == station_id].dropna(subset=['q'])\n",
    "        # dummy = amax_tab[amax_tab['id'] == station_id].dropna(subset=['q'])\n",
    "        # dummy = amax_tab[amax_tab['id'] == station_id]\n",
    "        \n",
    "        if len(dummy) < 10:\n",
    "            continue  # exclude short series\n",
    "        \n",
    "        # megafloods criteria:\n",
    "        \n",
    "        # dummy_quant = dummy['q'].quantile([0.25, 0.75])\n",
    "        # dummy_upper = dummy_quant.loc[0.75] + 3 * (dummy_quant.loc[0.75] - dummy_quant.loc[0.25])\n",
    "        # index_outlier = dummy[dummy['q'] >= dummy_upper].index\n",
    "\n",
    "        # 1. outlier \n",
    "        dummy_sorted = sorted(dummy[\"q\"])\n",
    "        n = len(dummy_sorted)\n",
    "        q1_index = int(0.25 * n) \n",
    "        q3_index = int(0.75 * n) \n",
    "        q1 = dummy_sorted[q1_index]\n",
    "        q3 = dummy_sorted[q3_index]\n",
    "        dummy_upper = q3 + 3 * (q3 - q1)\n",
    "        # # 分位数计算，很怪  还行\n",
    "\n",
    "        index_outlier = dummy[dummy['q'] >= dummy_upper].index\n",
    "        if len(index_outlier) == 0:\n",
    "            continue\n",
    "        \n",
    "        index_MF = []\n",
    "        for tt in index_outlier:\n",
    "            ty = dummy['year'][tt]\n",
    "            # nyears_before = len(dummy[dummy['year'] < ty])   # 可能存在的问题 在下一行改了之后并没有明显影响\n",
    "            nyears_before = len(dummy[dummy['year'] < ty]['q'])\n",
    "            \n",
    "            # 3. after start.year and with at least 20 years of data before event\n",
    "            if nyears_before < 20 or ty < start_year:\n",
    "                continue\n",
    "            \n",
    "        # # index_MF = []  # 存储异常事件的索引\n",
    "\n",
    "        # # for tt in range(1, len(dummy)):\n",
    "        #     qmax_old = dummy[dummy['year'] <= ty] ['q'] # 根据年份选择流量值的子集\n",
    "\n",
    "        # # # 计算L-moments\n",
    "        # # lmoments = lm3.lmom_ratios(qmax_old.values)\n",
    "\n",
    "        # # 使用L-moments进行GEV分布拟合\n",
    "        #     params = lm.pelgev(qmax_old)\n",
    "        \n",
    "\n",
    "        # # 使用pextRemes计算概率\n",
    "        #     neP = genextreme.cdf(qmax_old,*params)\n",
    "        #     RP = 1 / (1 - neP)\n",
    "        #     RP[np.isinf(RP)] = 10**5\n",
    "        #     rRP = RP[-1] / max(RP[:-1])\n",
    "\n",
    "        #     if rRP > 3:\n",
    "        #         index_MF.append(tt)\n",
    "\n",
    "        # index_outlier = index_MF[1:]\n",
    "\n",
    "            # 2. surprising (Tl/Tsl > 3) record-breaking event\n",
    "            qmax_old = dummy[dummy['year'] <= ty]['q']    # 同上的问题\n",
    "            # 计算概率分布函数\n",
    "           \n",
    "\n",
    "            local_gev = genextreme.fit(qmax_old)\n",
    "            neP = 1-genextreme.pdf(qmax_old, *local_gev)   # 方法计算对吗  这是真的大问题\n",
    "           \n",
    "            RP = 1 / (1 - neP)\n",
    "            RP[RP == float('inf')] = 10 ** 5\n",
    "            rRP = RP[-1] / max(RP[:-1])\n",
    "            if rRP > 3:\n",
    "                index_MF.append(tt)\n",
    "                \n",
    "        index_outlier = index_MF\n",
    "        if len(index_MF) > 0:\n",
    "            for oo in index_MF:\n",
    "                nn += 1\n",
    "                try:\n",
    "                    megafloods_df.loc[nn] = [stations_tab['id'][i], stations_tab['lat'][i], stations_tab['lon'][i],\n",
    "                                         float(stations_tab['area'][i]), dummy['q'][oo], dummy['year'][oo],\n",
    "                                         pd.to_datetime(\n",
    "                                             f\"{int(dummy['year'][oo])}-{int(dummy['month'][oo])}-{int(dummy['day'][oo])}\")]\n",
    "                except ValueError:\n",
    "                    megafloods_df.loc[nn] = [stations_tab['id'][i], stations_tab['lat'][i], stations_tab['lon'][i],\n",
    "                                         float(stations_tab['area'][i]), dummy['q'][oo], dummy['year'][oo],-1]\n",
    "    \n",
    "    return megafloods_df\n",
    "\n",
    "stations = pd.read_csv(\"./gauges.csv\")\n",
    "flood_data = pd.read_csv(\"./floods.csv\")\n",
    "megafloods_df = find_mf_func(stations,flood_data,2000)\n",
    "# print(find_mf_func(stations,flood_data,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二部分\n",
    "# 1.平均值和最大值\n",
    "import pandas as pd\n",
    "\n",
    "stations = pd.read_csv(\"./gauges.csv\")\n",
    "flood_data = pd.read_csv(\"./floods.csv\")\n",
    "\n",
    "\n",
    "# 计算平均值和最大值\n",
    "mean_q = flood_data.iloc[:, 1].groupby(flood_data['id']).mean().round(5)\n",
    "max_q = flood_data.iloc[:, 1].groupby(flood_data['id']).max()\n",
    "\n",
    "# 将结果赋给 stations 数据框\n",
    "stations['mean_q'] = stations['id'].map(mean_q)\n",
    "stations['max_q'] = stations['id'].map(max_q)\n",
    "\n",
    "# print(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二部分1：五个区域的包络曲线\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def env_func(stations_tab, amax_tab):\n",
    "    # Initialize an empty DataFrame for the results\n",
    "    columns = ['slope_qr_median', 'intercept_qr_median', 'slope_qr_env', 'intercept_qr_env', 'intercept_env', 'n_stat_years']\n",
    "    ENV_tab = pd.DataFrame(columns=columns, index=range(1, 6))\n",
    "    \n",
    "    for rr in range(1, 6):\n",
    "        # print(rr)     打印这个东西干嘛?\n",
    "\n",
    "        # Filter stations and amax data by region\n",
    "        dummy_stations = stations_tab[stations_tab['regions1'] == rr]\n",
    "        dummy_q = amax_tab[amax_tab['id'].isin(dummy_stations['id'])]\n",
    "        dummy_q = dummy_q[dummy_q['q'] > 0]\n",
    "        dummy_q['qlog10'] = np.log10(dummy_q['q'])\n",
    "        dummy_q['Alog10'] = np.nan\n",
    "\n",
    "        # Assign area log10 values\n",
    "        for i in dummy_stations['id']:\n",
    "            dummy_q.loc[dummy_q['id'] == i, 'Alog10'] = np.log10(dummy_stations.loc[dummy_stations['id'] == i, 'area'].values[0])\n",
    "\n",
    "        # Quantile regression for tau=0.5\n",
    "        model_median = smf.quantreg('qlog10 ~ Alog10', dummy_q).fit(q=0.5)\n",
    "        ENV_tab.at[rr, 'slope_qr_median'] = model_median.params['Alog10']\n",
    "        ENV_tab.at[rr, 'intercept_qr_median'] = model_median.params['Intercept']\n",
    "\n",
    "        # Quantile regression for tau=0.999\n",
    "        model_env = smf.quantreg('qlog10 ~ Alog10', dummy_q).fit(q=0.999)\n",
    "        ENV_tab.at[rr, 'slope_qr_env'] = model_env.params['Alog10']\n",
    "        ENV_tab.at[rr, 'intercept_qr_env'] = model_env.params['Intercept']\n",
    "\n",
    "        # Calculate intercept of envelope\n",
    "        intercept = -ENV_tab.at[rr, 'slope_qr_env'] * np.log10(dummy_stations['area']) + np.log10(dummy_stations['max_q'])\n",
    "        ENV_tab.at[rr, 'intercept_env'] = intercept.max()\n",
    "\n",
    "        # Number of station years\n",
    "        ENV_tab.at[rr, 'n_stat_years'] = len(dummy_q)\n",
    "\n",
    "    return ENV_tab\n",
    "# stations = pd.read_csv(\"./gauges.csv\")\n",
    "# flood_data = pd.read_csv(\"./floods.csv\")\n",
    "# print(env_func(stations, flood_data))\n",
    "ENV_reg = env_func(stations, flood_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Slope_Envelope Intercept_Envelope Intercept_Envelope_1000km2  \\\n",
      "1      -0.074694           0.359792                   1.366811   \n",
      "2      -0.382777           1.485215                   2.172128   \n",
      "3      -0.566036           2.419434                   5.264131   \n",
      "4      -0.270014           1.487376                   4.756991   \n",
      "5      -0.337627           1.486842                   2.978257   \n",
      "\n",
      "  Slope_median_regression Intercept_median_regression  \n",
      "1               -0.091551                   -0.879714  \n",
      "2               -0.217651                   -0.546294  \n",
      "3               -0.284169                   -0.021829  \n",
      "4               -0.199583                   -0.177523  \n",
      "5               -0.146376                   -0.647347  \n"
     ]
    }
   ],
   "source": [
    "# 第二部分2：EDT1 and EDT2\n",
    "# 未订正\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log10\n",
    "\n",
    "# EDT 1\n",
    "def ext_data_tab1_func(envelope_tab):\n",
    "    ExtDataTable1 = pd.DataFrame(np.nan, index=envelope_tab.index, columns=[\"Slope_Envelope\",\"Intercept_Envelope\",\"Intercept_Envelope_1000km2\",\"Slope_median_regression\",\"Intercept_median_regression\"])\n",
    "    ExtDataTable1[\"Slope_Envelope\"] = envelope_tab[\"slope_qr_env\"]\n",
    "    ExtDataTable1[\"Intercept_Envelope\"] = envelope_tab[\"intercept_env\"]\n",
    "    ExtDataTable1[\"Intercept_Envelope_1000km2\"] = 10 ** (envelope_tab[\"intercept_env\"] + envelope_tab[\"slope_qr_env\"] * log10(1000))\n",
    "    ExtDataTable1[\"Slope_median_regression\"] = envelope_tab[\"slope_qr_median\"]\n",
    "    ExtDataTable1[\"Intercept_median_regression\"] = envelope_tab[\"intercept_qr_median\"]\n",
    "    ExtDataTable1 = ExtDataTable1.round(2)\n",
    "    return ExtDataTable1\n",
    "\n",
    "ExtDataTable1 = ext_data_tab1_func(ENV_reg)\n",
    "print(ExtDataTable1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number_of_gauges  Avg_Ratio_max_mean_Q  Number_of_targets  \\\n",
      "1             671.0                  2.09                7.0   \n",
      "2            3660.0                  3.25              262.0   \n",
      "3             938.0                  3.50               21.0   \n",
      "4            1240.0                  2.83               80.0   \n",
      "5            1514.0                  2.36              121.0   \n",
      "\n",
      "   Percentage_of_targets  Avg_Ratio_max_mean_Q_targets  \n",
      "1                    1.0                          2.92  \n",
      "2                    7.2                          5.40  \n",
      "3                    2.2                          5.39  \n",
      "4                    6.5                          4.03  \n",
      "5                    8.0                          3.44  \n"
     ]
    }
   ],
   "source": [
    "# EDT 2\n",
    "ExtDataTable2 = pd.DataFrame(np.nan, index=range(1, 6), columns=[\"Number_of_gauges\",\"Avg_Ratio_max_mean_Q\",\"Number_of_targets\", \"Percentage_of_targets\",\"Avg_Ratio_max_mean_Q_targets\"])\n",
    "for rr in range(1, 6):\n",
    "    dummy_stations = stations[stations[\"regions1\"] == rr]\n",
    "    ExtDataTable2.loc[rr, \"Number_of_gauges\"] = dummy_stations.shape[0]\n",
    "    ExtDataTable2.loc[rr, \"Avg_Ratio_max_mean_Q\"] = (dummy_stations[\"max_q\"] / dummy_stations[\"mean_q\"]).mean().round(2)\n",
    "    dummy_mf = dummy_stations[dummy_stations[\"id\"].isin(megafloods_df[\"id\"])]\n",
    "    ExtDataTable2.loc[rr, \"Number_of_targets\"] = dummy_mf.shape[0]\n",
    "    ExtDataTable2.loc[rr, \"Percentage_of_targets\"] = (ExtDataTable2.loc[rr, \"Number_of_targets\"] * 100 / ExtDataTable2.loc[rr, \"Number_of_gauges\"]).round(1)\n",
    "    ExtDataTable2.loc[rr, \"Avg_Ratio_max_mean_Q_targets\"] = (dummy_mf[\"max_q\"] / dummy_mf[\"mean_q\"]).mean().round(2)\n",
    "\n",
    "del dummy_mf, dummy_stations\n",
    "\n",
    "print(ExtDataTable2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id    lat    lon     area  regions1  regions2   mean_q    max_q\n",
      "0        1  40.61  19.66   6708.0         3         3  0.23901  0.58069\n",
      "1        2  48.40  15.44  94762.0         2         4  0.05648  0.11537\n",
      "2        3  47.29   9.68   4626.0         4         4  0.20537  0.30831\n",
      "3        4  47.02   9.92    145.0         4         4  0.13750  0.26087\n",
      "4        5  47.04   9.95     39.3         4         4  0.31597  1.15013\n",
      "...    ...    ...    ...      ...       ...       ...      ...      ...\n",
      "8018  8019  46.93  36.86   1627.0         2         2  0.01490  0.13580\n",
      "8019  8020  47.13  37.59   3722.0         2         2  0.01724  0.15784\n",
      "8020  8021  47.46  37.45    194.0         2         2  0.03520  0.48969\n",
      "8021  8022  47.25  37.23    269.0         2         2  0.06849  0.51111\n",
      "8022  8023  48.10  38.11    272.0         2         2  0.04906  0.20588\n",
      "\n",
      "[8023 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2000:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.055616  0.278194      NaN\n",
      "2        3  47.29   9.68   4626.0       NaN       NaN      NaN\n",
      "3        4  47.02   9.92    145.0  0.127732  0.412555      NaN\n",
      "4        5  47.04   9.95     39.3  0.329941  0.383054      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.011363  1.238913      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.015584  0.851532      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.024747  0.960986      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.053101  0.958687      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.061789  0.849174      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2001:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.055523  0.277902      NaN\n",
      "2        3  47.29   9.68   4626.0       NaN       NaN      NaN\n",
      "3        4  47.02   9.92    145.0  0.127669  0.395188      NaN\n",
      "4        5  47.04   9.95     39.3  0.328693  0.379438      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.009831   1.20238      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.015316  0.862005      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.024293   0.97269      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.052021  0.973843      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.060512  0.866277      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2002:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.055503  0.276731      NaN\n",
      "2        3  47.29   9.68   4626.0       NaN       NaN      NaN\n",
      "3        4  47.02   9.92    145.0  0.142015  0.420739      NaN\n",
      "4        5  47.04   9.95     39.3  0.327517  0.375905      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.009601  1.223483      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.015088    0.8691      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.023849  0.984664      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.050988  0.988666      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.059234  0.884691      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2003:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056052  0.291148      NaN\n",
      "2        3  47.29   9.68   4626.0       NaN       NaN      NaN\n",
      "3        4  47.02   9.92    145.0  0.132896  0.374996      NaN\n",
      "4        5  47.04   9.95     39.3  0.326406  0.372453      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.009383  1.244181      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.014842  0.878944      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.023404   0.99789      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.050079  1.000391      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.058015  0.902566      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2004:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.055912   0.29173      NaN\n",
      "2        3  47.29   9.68   4626.0       NaN       NaN      NaN\n",
      "3        4  47.02   9.92    145.0  0.127289   0.41259      NaN\n",
      "4        5  47.04   9.95     39.3  0.321078  0.386527      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.011496  1.284676      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.015025  0.861395      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.024178  0.975342      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.052598  0.988337      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.057871  0.894367      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2005:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0   0.05583   0.29125      NaN\n",
      "2        3  47.29   9.68   4626.0       NaN       NaN      NaN\n",
      "3        4  47.02   9.92    145.0  0.126321  0.403643      NaN\n",
      "4        5  47.04   9.95     39.3  0.322137  0.380679      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.010182  1.261343      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.014821  0.867594      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.023919   0.97656      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.051813  0.996029      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.057106  0.900298      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2006:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.055937  0.290095      NaN\n",
      "2        3  47.29   9.68   4626.0  0.214204  0.282428      NaN\n",
      "3        4  47.02   9.92    145.0  0.139813  0.440118      NaN\n",
      "4        5  47.04   9.95     39.3  0.328949  0.389496      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.010021  1.270303      NaN\n",
      "8019  8020  47.13  37.59   3722.0   0.01462  0.874076      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.023531   0.98688      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.050769  1.013415      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.056095  0.914128      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2007:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056059  0.289096      NaN\n",
      "2        3  47.29   9.68   4626.0  0.209556  0.284051      NaN\n",
      "3        4  47.02   9.92    145.0  0.135818  0.458297      NaN\n",
      "4        5  47.04   9.95     39.3  0.324338  0.400038      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.012652  1.301202      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.014478  0.875293      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.023812  0.967195      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.050377  1.010917      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.055403  0.919259      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2008:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056222  0.288639      NaN\n",
      "2        3  47.29   9.68   4626.0  0.206491  0.280354      NaN\n",
      "3        4  47.02   9.92    145.0  0.132109  0.475265      NaN\n",
      "4        5  47.04   9.95     39.3  0.319264  0.413806      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.011225  1.280117      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.014263  0.884569      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.023517  0.971905      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.049406  1.027556      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.054323  0.937267      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2009:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0   0.05611  0.288738      NaN\n",
      "2        3  47.29   9.68   4626.0  0.211493  0.275452      NaN\n",
      "3        4  47.02   9.92    145.0  0.134889  0.463507      NaN\n",
      "4        5  47.04   9.95     39.3   0.31772  0.412011      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0   0.00997  1.262075      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.014052  0.893982      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.023085  0.987346      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.048461  1.044374      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.053383  0.951454      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2010:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056328  0.289372      NaN\n",
      "2        3  47.29   9.68   4626.0  0.210725  0.266723      NaN\n",
      "3        4  47.02   9.92    145.0  0.137039  0.451507      NaN\n",
      "4        5  47.04   9.95     39.3  0.315066  0.414239      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.009785  1.278103      NaN\n",
      "8019  8020  47.13  37.59   3722.0   0.01387   0.90077      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.022672  1.002162      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.047555  1.060812      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.052447  0.966457      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2011:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056484  0.288876      NaN\n",
      "2        3  47.29   9.68   4626.0  0.206775  0.272157      NaN\n",
      "3        4  47.02   9.92    145.0  0.137502  0.440378      NaN\n",
      "4        5  47.04   9.95     39.3   0.30268  0.375806      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.009713  1.274938      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.013699  0.906749      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.022451  1.003706      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.047721  1.046309      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.051902  0.969566      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2012:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056484  0.288876      NaN\n",
      "2        3  47.29   9.68   4626.0  0.204689  0.269108      NaN\n",
      "3        4  47.02   9.92    145.0  0.137502  0.440378      NaN\n",
      "4        5  47.04   9.95     39.3  0.307396  0.433183      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0   0.00957  1.284525      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.013515   0.91493      NaN\n",
      "8020  8021  47.46  37.45    194.0   0.02211  1.014412      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.046901   1.06064      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.051439  0.970754      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2013:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056484  0.288876      NaN\n",
      "2        3  47.29   9.68   4626.0  0.207191  0.262403      NaN\n",
      "3        4  47.02   9.92    145.0  0.137502  0.440378      NaN\n",
      "4        5  47.04   9.95     39.3  0.305039  0.434859      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.009418  1.296584      NaN\n",
      "8019  8020  47.13  37.59   3722.0   0.01335  0.921173      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.021747  1.027626      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.046121  1.074289      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.050737  0.979675      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2014:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056484  0.288876      NaN\n",
      "2        3  47.29   9.68   4626.0  0.205637  0.258819      NaN\n",
      "3        4  47.02   9.92    145.0  0.137502  0.440378      NaN\n",
      "4        5  47.04   9.95     39.3  0.306182  0.429349      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.008375  1.261024      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.013158  0.931473      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.021369  1.043026      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.045338  1.088918      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.049874  0.995033      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2015:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056484  0.288876      NaN\n",
      "2        3  47.29   9.68   4626.0  0.208074  0.253876      NaN\n",
      "3        4  47.02   9.92    145.0  0.137502  0.440378      NaN\n",
      "4        5  47.04   9.95     39.3  0.302751  0.436638      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.006632  1.066606      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.013038  0.933473      NaN\n",
      "8020  8021  47.46  37.45    194.0  0.021039  1.055078      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.044578  1.103551      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.049056  1.009541      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2016:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056484  0.288876      NaN\n",
      "2        3  47.29   9.68   4626.0  0.205367  0.257207      NaN\n",
      "3        4  47.02   9.92    145.0  0.137502  0.440378      NaN\n",
      "4        5  47.04   9.95     39.3  0.298951  0.446426      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.006533  1.076855      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.012854  0.943778      NaN\n",
      "8020  8021  47.46  37.45    194.0   0.02068  1.070567      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.043851  1.117606      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.049056  1.009541      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2017:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056484  0.288876      NaN\n",
      "2        3  47.29   9.68   4626.0  0.205367  0.257207      NaN\n",
      "3        4  47.02   9.92    145.0  0.137502  0.440378      NaN\n",
      "4        5  47.04   9.95     39.3  0.298951  0.446426      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.006533  1.076855      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.012854  0.943778      NaN\n",
      "8020  8021  47.46  37.45    194.0   0.02068  1.070567      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.043851  1.117606      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.049056  1.009541      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2018:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056484  0.288876      NaN\n",
      "2        3  47.29   9.68   4626.0  0.205367  0.257207      NaN\n",
      "3        4  47.02   9.92    145.0  0.137502  0.440378      NaN\n",
      "4        5  47.04   9.95     39.3  0.298951  0.446426      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.006533  1.076855      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.012854  0.943778      NaN\n",
      "8020  8021  47.46  37.45    194.0   0.02068  1.070567      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.043851  1.117606      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.049056  1.009541      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2019:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056484  0.288876      NaN\n",
      "2        3  47.29   9.68   4626.0  0.205367  0.257207      NaN\n",
      "3        4  47.02   9.92    145.0  0.137502  0.440378      NaN\n",
      "4        5  47.04   9.95     39.3  0.298951  0.446426      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.006533  1.076855      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.012854  0.943778      NaN\n",
      "8020  8021  47.46  37.45    194.0   0.02068  1.070567      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.043851  1.117606      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.049056  1.009541      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2020:         id    lat    lon     area       MHQ        CV  MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835      NaN\n",
      "1        2  48.40  15.44  94762.0  0.056484  0.288876      NaN\n",
      "2        3  47.29   9.68   4626.0  0.205367  0.257207      NaN\n",
      "3        4  47.02   9.92    145.0  0.137502  0.440378      NaN\n",
      "4        5  47.04   9.95     39.3  0.298951  0.446426      NaN\n",
      "...    ...    ...    ...      ...       ...       ...      ...\n",
      "8018  8019  46.93  36.86   1627.0  0.006533  1.076855      NaN\n",
      "8019  8020  47.13  37.59   3722.0  0.012854  0.943778      NaN\n",
      "8020  8021  47.46  37.45    194.0   0.02068  1.070567      NaN\n",
      "8021  8022  47.25  37.23    269.0  0.043851  1.117606      NaN\n",
      "8022  8023  48.10  38.11    272.0  0.049056  1.009541      NaN\n",
      "\n",
      "[8023 rows x 7 columns], 2021:         id    lat    lon     area       MHQ        CV   MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835  0.673903\n",
      "1        2  48.40  15.44  94762.0  0.056484  0.288876  0.305872\n",
      "2        3  47.29   9.68   4626.0  0.205367  0.257207  0.528363\n",
      "3        4  47.02   9.92    145.0  0.137502  0.440378  0.150688\n",
      "4        5  47.04   9.95     39.3  0.298951  0.446426  0.237484\n",
      "...    ...    ...    ...      ...       ...       ...       ...\n",
      "8018  8019  46.93  36.86   1627.0  0.006533  1.076855  0.012992\n",
      "8019  8020  47.13  37.59   3722.0  0.012854  0.943778  0.031346\n",
      "8020  8021  47.46  37.45    194.0   0.02068  1.070567  0.024349\n",
      "8021  8022  47.25  37.23    269.0  0.043851  1.117606  0.055962\n",
      "8022  8023  48.10  38.11    272.0  0.049056  1.009541  0.062776\n",
      "\n",
      "[8023 rows x 7 columns], 2022:         id    lat    lon     area       MHQ        CV   MHQ_100\n",
      "0        1  40.61  19.66   6708.0  0.239013  0.471835  0.673783\n",
      "1        2  48.40  15.44  94762.0  0.056484  0.288876  0.305783\n",
      "2        3  47.29   9.68   4626.0  0.205367  0.257207  0.528277\n",
      "3        4  47.02   9.92    145.0  0.137502  0.440378  0.150686\n",
      "4        5  47.04   9.95     39.3  0.298951  0.446426  0.237494\n",
      "...    ...    ...    ...      ...       ...       ...       ...\n",
      "8018  8019  46.93  36.86   1627.0  0.006533  1.076855  0.012990\n",
      "8019  8020  47.13  37.59   3722.0  0.012854  0.943778  0.031341\n",
      "8020  8021  47.46  37.45    194.0   0.02068  1.070567  0.024349\n",
      "8021  8022  47.25  37.23    269.0  0.043851  1.117606  0.055960\n",
      "8022  8023  48.10  38.11    272.0  0.049056  1.009541  0.062773\n",
      "\n",
      "[8023 rows x 7 columns]}\n"
     ]
    }
   ],
   "source": [
    "# 第三部分:try2 \n",
    "# 似乎流程正确，但总感觉差点什么（没有完全显示而已）\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def mhq_cv_func(stations_df, amax_df, target_years):\n",
    "    EU_MHQ_CV_list = {}\n",
    "    for target_year in target_years:\n",
    "        # print(target_year)\n",
    "        EU_MHQ_CV = pd.DataFrame(columns=[\"MHQ\", \"CV\", \"MHQ_100\"])\n",
    "        EU_MHQ_CV = pd.concat([stations_df[['id', 'lat', 'lon', 'area']], EU_MHQ_CV], axis=1)\n",
    "        for index, station in EU_MHQ_CV.iterrows():\n",
    "            dummy_df = amax_df[(amax_df['id'] == station['id']) & (amax_df['year'] < target_year)]\n",
    "            dummy_df = dummy_df.dropna(subset=['q'])\n",
    "            # print(f'Year: {target_year}, Station ID: {station[\"id\"]}, Dummy DF Length: {len(dummy_df)}')\n",
    "            if len(dummy_df) < 10: \n",
    "                continue\n",
    "            dummy_Q = dummy_df['q'].values\n",
    "            dummy_Y = dummy_df['year'].values\n",
    "            # Exclude outliers\n",
    "            Q1 = np.quantile(dummy_Q, 0.25)\n",
    "            Q3 = np.quantile(dummy_Q, 0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            upper_bound = Q3 + 3 * IQR\n",
    "            filtered_indices = dummy_Q <= upper_bound\n",
    "            dummy_Q = dummy_Q[filtered_indices]\n",
    "            dummy_Y = dummy_Y[filtered_indices]\n",
    "            EU_MHQ_CV.at[index, 'MHQ'] = np.mean(dummy_Q)\n",
    "            EU_MHQ_CV.at[index, 'CV'] = np.std(dummy_Q) / EU_MHQ_CV.at[index, 'MHQ']\n",
    "        # Normalize MHQ to 100km2\n",
    "        EU_MHQ_CV['MHQ'] = EU_MHQ_CV['MHQ'].replace(0, np.nan)  # Replace 0 with NaN to avoid log(0) errors\n",
    "        EU_MHQ_CV['MHQ_log'] = np.log10(EU_MHQ_CV['MHQ'].astype(float))  # Corrected\n",
    "        stations_df = stations_df.dropna(subset=['area'])\n",
    "        EU_MHQ_CV['area_log'] = np.log10(stations_df['area'].astype(float))  # Corrected\n",
    "        area_regr = np.polyfit(EU_MHQ_CV['area_log'], EU_MHQ_CV['MHQ_log'], 1)\n",
    "        EU_MHQ_CV['MHQ_100'] = 10 ** (EU_MHQ_CV['MHQ_log'] - area_regr[0] * EU_MHQ_CV['area_log'] + area_regr[0] * np.log10(100))\n",
    "        # 删除还有NaN的行\n",
    "        # EU_MHQ_CV = EU_MHQ_CV.dropna(subset=['area', 'MHQ_100', 'CV'])\n",
    "        EU_MHQ_CV_list[target_year] = EU_MHQ_CV.drop(columns=['MHQ_log', 'area_log'])\n",
    "    return EU_MHQ_CV_list\n",
    "\n",
    "\n",
    "EU_MHQ_CV_list = mhq_cv_func(stations, flood_data, range(2000, 2023))\n",
    "print(EU_MHQ_CV_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id    LAT    LON    area     Q_MF  year_MF              date_MF\n",
      "1      10.0  47.17   9.78    95.5  1.15183   2005.0  2005-08-23 00:00:00\n",
      "2      11.0  47.29   9.85    33.4  7.84431   2005.0  2005-08-22 00:00:00\n",
      "3      14.0  47.26   9.96    41.7  3.83693   2005.0  2005-08-23 00:00:00\n",
      "4      15.0  47.23   9.78   148.0  2.40455   2005.0  2005-08-23 00:00:00\n",
      "5      17.0  47.36  10.19    31.1  3.89068   2005.0  2005-08-23 00:00:00\n",
      "..      ...    ...    ...     ...      ...      ...                  ...\n",
      "503  7759.0  48.69  23.48     3.2  1.38125   2015.0  2015-11-21 00:00:00\n",
      "504  7843.0  49.14  23.03   137.0  1.95588   2008.0  2008-07-25 00:00:00\n",
      "505  7855.0  49.41  22.93   384.0  1.87760   2008.0  2008-07-25 00:00:00\n",
      "506  7860.0  48.21  24.98  1501.0  1.00000   2008.0  2008-07-26 00:00:00\n",
      "507  7862.0  48.56  24.76    18.1  4.79006   2006.0  2006-06-30 00:00:00\n",
      "\n",
      "[507 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(megafloods_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanchan\\AppData\\Local\\Temp\\ipykernel_19020\\3713120121.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ENVELOPE_param = pd.concat([ENVELOPE_param, new_df], ignore_index=True)\n",
      "C:\\Users\\hanchan\\AppData\\Local\\Temp\\ipykernel_19020\\3713120121.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ENVELOPE_param = pd.concat([ENVELOPE_param, new_df], ignore_index=True)\n",
      "C:\\Users\\hanchan\\AppData\\Local\\Temp\\ipykernel_19020\\3713120121.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  DONORS = pd.concat([DONORS, new_df], ignore_index=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot assemble the datetimes: unconverted data remains when parsing with format \"%Y%m%d\": \"05\", at position 3. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1259\u001b[0m, in \u001b[0;36m_assemble_from_unit_mappings\u001b[1;34m(arg, errors, utc)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1259\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1112\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1112\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1113\u001b[0m     result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:488\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64ns(\n\u001b[0;32m    491\u001b[0m     arg,\n\u001b[0;32m    492\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    497\u001b[0m )\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:519\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03mCall array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 519\u001b[0m result, timezones \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m tz \u001b[38;5;129;01min\u001b[39;00m timezones):\n",
      "File \u001b[1;32mstrptime.pyx:534\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:359\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unconverted data remains when parsing with format \"%Y%m%d\": \"05\", at position 3. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 145\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RES\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# 示例数据加载和函数调用部分\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# 请确保你已经准备好了mf_list, stations_tab, target_years, amax_tab, mhq_cv_tab, env_tab这些变量的数据\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# 例如:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# mhq_cv_tab = {2000: pd.DataFrame(...), 2001: pd.DataFrame(...), ...}\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# env_tab = pd.read_csv('ENV_reg.csv')\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m MF_predicted \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_mf_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmegafloods_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2002\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2003\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2004\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2006\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2007\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2008\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2009\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2010\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2011\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2012\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2013\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2014\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2015\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2016\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2017\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2018\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2019\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2020\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2021\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2022\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflood_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEU_MHQ_CV_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mENV_reg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[55], line 63\u001b[0m, in \u001b[0;36mpredict_mf_function\u001b[1;34m(mf_list, stations_tab, target_years, amax_tab, mhq_cv_tab, env_tab)\u001b[0m\n\u001b[0;32m     61\u001b[0m dummy \u001b[38;5;241m=\u001b[39m amax_tab[amax_tab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m row_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     62\u001b[0m QQ \u001b[38;5;241m=\u001b[39m dummy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m][dummy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m TARGET_YEAR]\n\u001b[1;32m---> 63\u001b[0m DD \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmonth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mday\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdummy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTARGET_YEAR\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m YY \u001b[38;5;241m=\u001b[39m dummy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m][dummy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m TARGET_YEAR]\n\u001b[0;32m     65\u001b[0m ind_SFOR \u001b[38;5;241m=\u001b[39m QQ\u001b[38;5;241m.\u001b[39midxmax()\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1115\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1113\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n\u001b[1;32m-> 1115\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_assemble_from_unit_mappings\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, Index):\n\u001b[0;32m   1117\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m _maybe_cache(arg, \u001b[38;5;28mformat\u001b[39m, cache, convert_listlike)\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1261\u001b[0m, in \u001b[0;36m_assemble_from_unit_mappings\u001b[1;34m(arg, errors, utc)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     values \u001b[38;5;241m=\u001b[39m to_datetime(values, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39merrors, utc\u001b[38;5;241m=\u001b[39mutc)\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assemble the datetimes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m units: \u001b[38;5;28mlist\u001b[39m[UnitChoices] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mns\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m units:\n",
      "\u001b[1;31mValueError\u001b[0m: cannot assemble the datetimes: unconverted data remains when parsing with format \"%Y%m%d\": \"05\", at position 3. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "# 第三部分：预测\n",
    "# 问题：没有用到“target_years”输入必定会少东西的咯\n",
    "# 当前：存在时间格式上的问题\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import datetime\n",
    "\n",
    "def predict_mf_function(mf_list, stations_tab, target_years, amax_tab, mhq_cv_tab, env_tab):\n",
    "    w1, w2, w3 = 1, 1, 1\n",
    "    DIST = 1\n",
    "\n",
    "    REG_MEGAFLOODS_list = []\n",
    "    REG_ENVELOPE_param = []\n",
    "    REG_DONORS_list = []\n",
    "\n",
    "    for REG in range(1, 6):  # loop on Regions\n",
    "        REG_list_stations = stations_tab[stations_tab['regions1'] == REG]['id']\n",
    "        MEGAFLOODS_list = mf_list[mf_list['id'].isin(REG_list_stations)]\n",
    "        DONORS_list = []\n",
    "        ENVELOPE_param = pd.DataFrame(columns=[\"slope\", \"intercept.region\", \"intercept.mf\", \"q.predicted\", \"n_statyears_donors\", \"n_stat_donors\"])\n",
    "\n",
    "        for i, row in MEGAFLOODS_list.iterrows():  # loop on megafloods\n",
    "            TARGET_YEAR = row['year_MF']\n",
    "            TARGET_Q = row['Q_MF']\n",
    "            TARGET_DATE = row['date_MF']\n",
    "            code_mf = row['id']\n",
    "            EU_MHQ_CV = mhq_cv_tab[TARGET_YEAR].dropna(subset=['MHQ'])\n",
    "            # print(EU_MHQ_CV)\n",
    "            # EU_MHQ_CV = EU_MHQ_CV[~EU_MHQ_CV['MHQ'].isna()]\n",
    "            EU_MHQ_CV = EU_MHQ_CV[EU_MHQ_CV['id'].isin(REG_list_stations)]\n",
    "    \n",
    "            # distances\n",
    "            ind_stat = EU_MHQ_CV[EU_MHQ_CV['id'] == code_mf].index[0]\n",
    "            # sd1 = zscore(np.log10(EU_MHQ_CV['area']))\n",
    "            # sd2 = zscore(np.log10(EU_MHQ_CV['MHQ_100']))\n",
    "            # sd3 = zscore(np.log10(EU_MHQ_CV['CV']))\n",
    "            # 先计算 log10\n",
    "            log_area = np.log10(EU_MHQ_CV['area'])\n",
    "            log_mhq_100 = np.log10(EU_MHQ_CV['MHQ_100'])\n",
    "            # 处理 NaN 值\n",
    "            EU_MHQ_CV['CV'].fillna(0, inplace=True)\n",
    "            log_cv = np.log10(EU_MHQ_CV['CV'].astype(float))\n",
    "\n",
    "            # 然后计算 zscore\n",
    "            sd1 = zscore(log_area)\n",
    "            sd2 = zscore(log_mhq_100)\n",
    "            sd3 = zscore(log_cv)\n",
    "\n",
    "            d1 = sd1 - sd1[ind_stat]\n",
    "            d2 = sd2 - sd2[ind_stat]\n",
    "            d3 = sd3 - sd3[ind_stat]\n",
    "            dd = np.sqrt((d1**2)*w1 + (d2**2)*w2 + (d3**2)*w3)\n",
    "            # donor group\n",
    "            group = EU_MHQ_CV['id'][dd <= DIST].values\n",
    "            stations_Reg_group = stations_tab[stations_tab['id'].isin(group)]\n",
    "            EU_MHQ_CV_group = EU_MHQ_CV[EU_MHQ_CV['id'].isin(group)]\n",
    "            # characteristics of donor catchments\n",
    "            DONORS = pd.DataFrame(columns=[\"id\", \"lat\", \"lon\", \"area\", \"SFOR.B\", \"SFOR.year.B\", \"SFOR.date.B\"])\n",
    "\n",
    "            for s, row_group in EU_MHQ_CV_group.iterrows():\n",
    "                dummy = amax_tab[amax_tab['id'] == row_group['id']]\n",
    "                QQ = dummy['q'][dummy['year'] < TARGET_YEAR]\n",
    "                DD = pd.to_datetime(dummy[['year', 'month', 'day']][dummy['year'] < TARGET_YEAR])\n",
    "                YY = dummy['year'][dummy['year'] < TARGET_YEAR]\n",
    "                ind_SFOR = QQ.idxmax()\n",
    "                # DONORS = DONORS.append({\n",
    "                #     \"id\": row_group['id'],\n",
    "                #     \"lat\": row_group['lat'],\n",
    "                #     \"lon\": row_group['lon'],\n",
    "                #     \"area\": row_group['area'],\n",
    "                #     \"SFOR.B\": QQ.loc[ind_SFOR],\n",
    "                #     \"SFOR.year.B\": YY.loc[ind_SFOR],\n",
    "                #     \"SFOR.date.B\": DD.loc[ind_SFOR]\n",
    "                # }, ignore_index=True)\n",
    "\n",
    "                # 假设row_group包含要添加的数据\n",
    "                new_data = {\n",
    "                    \"id\": row_group['id'],\n",
    "                    \"lat\": row_group['lat'],\n",
    "                    \"lon\": row_group['lon'],\n",
    "                    \"area\": row_group['area'],\n",
    "                    \"SFOR.B\": QQ.loc[ind_SFOR],\n",
    "                    \"SFOR.year.B\": YY.loc[ind_SFOR],\n",
    "                    \"SFOR.date.B\": DD.loc[ind_SFOR]\n",
    "                }\n",
    "\n",
    "                # 创建包含新数据的DataFrame\n",
    "                new_df = pd.DataFrame(new_data, index=[0])\n",
    "\n",
    "                # 使用pd.concat将新数据添加到DONORS中\n",
    "                DONORS = pd.concat([DONORS, new_df], ignore_index=True)\n",
    "\n",
    "\n",
    "            # envelope with region slope\n",
    "            slope = env_tab['slope_qr_env'][REG]\n",
    "            intercept_region = env_tab['intercept_qr_env'][REG]\n",
    "            intercept = -slope * np.log10(DONORS['area']) + np.log10(DONORS['SFOR.B'])\n",
    "            DONORS['intercept'] = intercept\n",
    "            DONORS = DONORS.sort_values(by='intercept', ascending=False)\n",
    "            DONORS_list.append(DONORS)\n",
    "\n",
    "            # 假设要添加的数据为new_data\n",
    "            new_df = pd.DataFrame({\n",
    "                \"slope\": slope,\n",
    "                \"intercept.region\": intercept_region,\n",
    "                \"intercept.mf\": intercept.max(),\n",
    "                \"q.predicted\": intercept.max() + slope * np.log10(row['area']),\n",
    "                \"n_statyears_donors\": amax_tab[amax_tab['id'].isin(DONORS['id'])].shape[0],\n",
    "                \"n_stat_donors\": len(group)\n",
    "            }, index=[0])\n",
    "\n",
    "            # 使用pd.concat将新数据添加到ENVELOPE_param中\n",
    "            ENVELOPE_param = pd.concat([ENVELOPE_param, new_df], ignore_index=True)\n",
    "\n",
    "            # ENVELOPE_param = ENVELOPE_param.append({\n",
    "            #     \"slope\": slope,\n",
    "            #     \"intercept.region\": intercept_region,\n",
    "            #     \"intercept.mf\": intercept.max(),\n",
    "            #     \"q.predicted\": intercept.max() + slope * np.log10(row['area']),\n",
    "            #     \"n_statyears_donors\": amax_tab[amax_tab['id'].isin(DONORS['id'])].shape[0],\n",
    "            #     \"n_stat_donors\": len(group)\n",
    "            # }, ignore_index=True)\n",
    "\n",
    "        REG_MEGAFLOODS_list.append(MEGAFLOODS_list)\n",
    "        REG_ENVELOPE_param.append(ENVELOPE_param)\n",
    "        REG_DONORS_list.append(DONORS_list)\n",
    "\n",
    "    RES = {\n",
    "        \"mf.list\": REG_MEGAFLOODS_list,\n",
    "        \"EC.param\": REG_ENVELOPE_param,\n",
    "        \"donor.list\": REG_DONORS_list\n",
    "    }\n",
    "    return RES\n",
    "\n",
    "# 示例数据加载和函数调用部分\n",
    "# 请确保你已经准备好了mf_list, stations_tab, target_years, amax_tab, mhq_cv_tab, env_tab这些变量的数据\n",
    "# 例如:\n",
    "# mf_list = pd.read_csv('megafloods.csv')\n",
    "# stations_tab = pd.read_csv('stations.csv')\n",
    "# target_years = np.array([2000, 2001, 2002, ..., 2022])\n",
    "# amax_tab = pd.read_csv('flood_data.csv')\n",
    "# mhq_cv_tab = {2000: pd.DataFrame(...), 2001: pd.DataFrame(...), ...}\n",
    "# env_tab = pd.read_csv('ENV_reg.csv')\n",
    "\n",
    "MF_predicted = predict_mf_function(megafloods_df, stations, np.array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]), flood_data, EU_MHQ_CV_list, ENV_reg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([ 842,  843,  844,  845,  846,  850,  851,  853,  855,  856,\\n       ...\\n       6929, 6930, 6931, 6933, 6934, 6935, 6936, 6937, 6938, 6939],\\n      dtype='int64', length=666)] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m     RES \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmf.list\u001b[39m\u001b[38;5;124m'\u001b[39m: REG_MEGAFLOODS_list, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEC.param\u001b[39m\u001b[38;5;124m'\u001b[39m: REG_ENVELOPE_param, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdonor.list\u001b[39m\u001b[38;5;124m'\u001b[39m: REG_DONORS_list}\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RES\n\u001b[1;32m---> 79\u001b[0m MF_predicted \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_mf_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmegafloods_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstations\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflood_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEU_MHQ_CV_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mENV_reg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(MF_predicted)\n",
      "Cell \u001b[1;32mIn[23], line 37\u001b[0m, in \u001b[0;36mpredict_mf_function\u001b[1;34m(mf_list, stations_tab, target_years, amax_tab, mhq_cv_tab, env_tab)\u001b[0m\n\u001b[0;32m     35\u001b[0m sd2 \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mlog10(EU_MHQ_CV_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMHQ_100\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mlog10(EU_MHQ_CV_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMHQ_100\u001b[39m\u001b[38;5;124m'\u001b[39m]))) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(np\u001b[38;5;241m.\u001b[39mlog10(EU_MHQ_CV_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMHQ_100\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     36\u001b[0m sd3 \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mlog10(EU_MHQ_CV_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCV\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mlog10(EU_MHQ_CV_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCV\u001b[39m\u001b[38;5;124m'\u001b[39m]))) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(np\u001b[38;5;241m.\u001b[39mlog10(EU_MHQ_CV_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCV\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m---> 37\u001b[0m d1 \u001b[38;5;241m=\u001b[39m sd1 \u001b[38;5;241m-\u001b[39m \u001b[43msd1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind_stat\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     38\u001b[0m d2 \u001b[38;5;241m=\u001b[39m sd2 \u001b[38;5;241m-\u001b[39m sd2[ind_stat]\n\u001b[0;32m     39\u001b[0m d3 \u001b[38;5;241m=\u001b[39m sd3 \u001b[38;5;241m-\u001b[39m sd3[ind_stat]\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\series.py:1072\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rows_with_mask(key)\n\u001b[1;32m-> 1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\series.py:1099\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;66;03m# We need to decide whether to treat this as a positional indexer\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m#  (i.e. self.iloc) or label-based (i.e. self.loc)\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_should_fallback_to_positional:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1102\u001b[0m             \u001b[38;5;66;03m# GH#50617\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.__getitem__ treating keys as positions is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1108\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1109\u001b[0m         )\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\indexing.py:1382\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\indexing.py:1322\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1324\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1325\u001b[0m )\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\indexing.py:1520\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1517\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1518\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1520\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index([ 842,  843,  844,  845,  846,  850,  851,  853,  855,  856,\\n       ...\\n       6929, 6930, 6931, 6933, 6934, 6935, 6936, 6937, 6938, 6939],\\n      dtype='int64', length=666)] are in the [index]\""
     ]
    }
   ],
   "source": [
    "# 同上问题：没有用到“target_years”输入必定会少东西的咯\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "\n",
    "def predict_mf_function(mf_list, stations_tab, target_years, amax_tab, mhq_cv_tab, env_tab):\n",
    "    w1 = 1\n",
    "    w2 = 1\n",
    "    w3 = 1\n",
    "    DIST = 1\n",
    "\n",
    "    REG_MEGAFLOODS_list = [None]*5\n",
    "    REG_ENVELOPE_param = [None]*5\n",
    "    REG_DONORS_list = [None]*5\n",
    "\n",
    "    for REG in range(5):\n",
    "        REG_list_stations = stations_tab['id'][stations_tab['regions1'] == REG]\n",
    "        MEGAFLOODS_list = mf_list[mf_list['id'].isin(REG_list_stations)]\n",
    "        DONORS_list = [None]*len(MEGAFLOODS_list)\n",
    "        ENVELOPE_param = pd.DataFrame(np.nan, index=range(len(MEGAFLOODS_list)), columns=['slope', 'intercept.region', 'intercept.mf', 'q.predicted', 'n_statyears_donors', 'n_stat_donors'])\n",
    "\n",
    "        for i in range(len(MEGAFLOODS_list)):\n",
    "            print(i)\n",
    "\n",
    "            TARGET_YEAR = MEGAFLOODS_list['year_MF'].iloc[i]\n",
    "            TARGET_Q = MEGAFLOODS_list['Q_MF'].iloc[i]\n",
    "            TARGET_DATE = MEGAFLOODS_list['date_MF'].iloc[i]\n",
    "            code_mf = MEGAFLOODS_list['id'].iloc[i]\n",
    "            EU_MHQ_CV = mhq_cv_tab[TARGET_YEAR]\n",
    "            EU_MHQ_CV = EU_MHQ_CV[~EU_MHQ_CV['MHQ'].isna()]\n",
    "            EU_MHQ_CV = EU_MHQ_CV[EU_MHQ_CV['id'].isin(REG_list_stations)]\n",
    "\n",
    "            ind_stat = EU_MHQ_CV['id']\n",
    "            sd1 = (np.log10(EU_MHQ_CV['area']) - np.mean(np.log10(EU_MHQ_CV['area']))) / np.std(np.log10(EU_MHQ_CV['area']))\n",
    "            sd2 = (np.log10(EU_MHQ_CV['MHQ_100']) - np.mean(np.log10(EU_MHQ_CV['MHQ_100']))) / np.std(np.log10(EU_MHQ_CV['MHQ_100']))\n",
    "            sd3 = (np.log10(EU_MHQ_CV['CV']) - np.mean(np.log10(EU_MHQ_CV['CV']))) / np.std(np.log10(EU_MHQ_CV['CV']))\n",
    "            d1 = sd1 - sd1[ind_stat]\n",
    "            d2 = sd2 - sd2[ind_stat]\n",
    "            d3 = sd3 - sd3[ind_stat]\n",
    "            dd = np.sqrt((d1**2)*w1 + (d2**2)*w2 + (d3**2)*w3)\n",
    "            group = EU_MHQ_CV['id'][dd <= DIST]\n",
    "            stations_Reg_group = stations_tab[stations_tab['id'].isin(group)]\n",
    "            EU_MHQ_CV_group = EU_MHQ_CV[EU_MHQ_CV['id'].isin(group)]\n",
    "\n",
    "            DONORS = pd.DataFrame(columns=['id', 'lat', 'lon', 'area', 'SFOR.B', 'SFOR.year.B', 'SFOR.date.B'])\n",
    "            for s in range(len(EU_MHQ_CV_group)):\n",
    "                DONORS.loc[s, 'id'] = EU_MHQ_CV_group['id'].iloc[s]\n",
    "                DONORS.loc[s, 'lat'] = EU_MHQ_CV_group['lat'].iloc[s]\n",
    "                DONORS.loc[s, 'lon'] = EU_MHQ_CV_group['lon'].iloc[s]\n",
    "                DONORS.loc[s, 'area'] = EU_MHQ_CV_group['area'].iloc[s]\n",
    "                dummy = amax_tab[amax_tab['id'] == EU_MHQ_CV_group['id'].iloc[s]]\n",
    "                QQ = dummy['q'][dummy['year'] < TARGET_YEAR]\n",
    "                DD = [datetime.strptime(f\"{y}-{m}-{d}\", \"%Y-%m-%d\") for y, m, d in zip(dummy['year'][dummy['year'] < TARGET_YEAR], dummy['month'][dummy['year'] < TARGET_YEAR], dummy['day'][dummy['year'] < TARGET_YEAR])]\n",
    "                YY = dummy['year'][dummy['year'] < TARGET_YEAR]\n",
    "                ind_SFOR = np.argmax(QQ)\n",
    "                if len(ind_SFOR) > 1:\n",
    "                    ind_SFOR = np.max(ind_SFOR)\n",
    "                DONORS.loc[s, 'SFOR.B'] = QQ[ind_SFOR]\n",
    "                DONORS.loc[s, 'SFOR.year.B'] = YY[ind_SFOR]\n",
    "                DONORS.loc[s, 'SFOR.date.B'] = DD[ind_SFOR]\n",
    "\n",
    "            ENVELOPE_param['slope'].iloc[i] = env_tab['slope_qr_env'].iloc[REG]\n",
    "            ENVELOPE_param['intercept.region'].iloc[i] = env_tab['intercept_qr_env'].iloc[REG]\n",
    "            intercept = -ENVELOPE_param['slope'].iloc[i]*np.log10(DONORS['area']) + np.log10(DONORS['SFOR.B'])\n",
    "            DONORS = DONORS.sort_values(by='intercept', ascending=False)\n",
    "            DONORS_list[i] = DONORS\n",
    "            ENVELOPE_param['intercept.mf'].iloc[i] = np.max(intercept)\n",
    "            ENVELOPE_param['q.predicted'].iloc[i] = ENVELOPE_param['intercept.mf'].iloc[i] + ENVELOPE_param['slope'].iloc[i]*np.log10(MEGAFLOODS_list['area'].iloc[i])\n",
    "            ENVELOPE_param['n_statyears_donors'].iloc[i] = len(amax_tab[amax_tab['id'].isin(DONORS['id'])])\n",
    "            ENVELOPE_param['n_stat_donors'].iloc[i] = len(group)\n",
    "\n",
    "        REG_MEGAFLOODS_list[REG] = MEGAFLOODS_list\n",
    "        REG_ENVELOPE_param[REG] = ENVELOPE_param\n",
    "        REG_DONORS_list[REG] = DONORS_list\n",
    "\n",
    "    RES = {'mf.list': REG_MEGAFLOODS_list, 'EC.param': REG_ENVELOPE_param, 'donor.list': REG_DONORS_list}\n",
    "    return RES\n",
    "\n",
    "MF_predicted = predict_mf_function(megafloods_df, stations,2000, flood_data, EU_MHQ_CV_list, ENV_reg)\n",
    "print(MF_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己研究 不完整\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "\n",
    "def predict_mf_function(mf_list, stations_tab, target_years, amax_tab, mhq_cv_tab, env_tab):\n",
    "\n",
    "    # Specify weights and distance limit for finding donor group\n",
    "    w1 = 1\n",
    "    w2 = 1\n",
    "    w3 = 1\n",
    "    DIST = 1\n",
    "    \n",
    "    # Prepare lists for saving results\n",
    "    reg_megafloods_list = [None] * 5  # List megafloods per each region\n",
    "    reg_envelope_param = [None] * 5  # Envelope parameters\n",
    "    reg_donors_list = [None] * 5  # Donor list per region per megaflood\n",
    "\n",
    "    for REG in range(5):  # loop on Regions\n",
    "        reg_list_stations = stations_tab.loc[stations_tab['regions1'] == REG, 'id']\n",
    "        megafloods_list = mf_list[mf_list['id'].isin(reg_list_stations)]\n",
    "        donors_list = []\n",
    "        envelope_param = pd.DataFrame(np.nan, index=range(len(megafloods_list)), columns=['slope', 'intercept.region', 'intercept.mf', 'q.predicted', 'n_statyears_donors', 'n_stat_donors'])\n",
    "\n",
    "        # Loop on megafloods\n",
    "        for i in range(len(megafloods_list)):\n",
    "            print(i)\n",
    "\n",
    "            target_year = megafloods_list.iloc[i]['year_MF']\n",
    "            target_q = megafloods_list.iloc[i]['Q_MF']\n",
    "            target_date = megafloods_list.iloc[i]['date_MF']\n",
    "            code_mf = megafloods_list.iloc[i]['id']\n",
    "\n",
    "            eu_mhq_cv = mhq_cv_tab[target_years.index(target_year)]\n",
    "            eu_mhq_cv = eu_mhq_cv.dropna(subset=['MHQ'])\n",
    "            eu_mhq_cv = eu_mhq_cv[eu_mhq_cv['id'].isin(reg_list_stations)]\n",
    "\n",
    "            # distances\n",
    "            ind_stat = eu_mhq_cv[eu_mhq_cv['id'] == code_mf].index[0]\n",
    "            sd1 = (np.log10(eu_mhq_cv['area']) - np.mean(np.log10(eu_mhq_cv['area']))) / np.std(np.log10(eu_mhq_cv['area']))\n",
    "            sd2 = (np.log10(eu_mhq_cv['MHQ_100']) - np.mean(np.log10(eu_mhq_cv['MHQ_100']))) / np.std(np.log10(eu_mhq_cv['MHQ_100']))\n",
    "            sd3 = (np.log10(eu_mhq_cv['CV']) - np.mean(np.log10(eu_mhq_cv['CV']))) / np.std(np.log10(eu_mhq_cv['CV']))\n",
    "\n",
    "            d1 = sd1 - sd1[ind_stat]\n",
    "            d2 = sd2 - sd2[ind_stat]\n",
    "            d3 = sd3 - sd3[ind_stat]\n",
    "\n",
    "            dd = np.sqrt((d1**2)*w1 + (d2**2)*w2 + (d3**2)*w3)\n",
    "\n",
    "            # Donor group\n",
    "            group = eu_mhq_cv.loc[dd <= DIST, 'id']\n",
    "            stations_reg_group = stations_tab.loc[stations_tab['id'].isin(group)]\n",
    "            eu_mhq_cv_group = eu_mhq_cv.loc[eu_mhq_cv['id'].isin(group)]\n",
    "\n",
    "            # Continue adapting the rest of the code here...\n",
    "\n",
    "    # Set the appropriate names and return the result\n",
    "    reg_megafloods_list = dict(zip(range(1, 6), reg_meg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statistical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
