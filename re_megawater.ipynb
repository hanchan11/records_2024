{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "megafloods\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import genextreme\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "# Read CSV files into DataFrames\n",
    "stations = pd.read_csv(\"./gauges.csv\")\n",
    "flood_data = pd.read_csv(\"./floods.csv\")\n",
    "print(stations)\n",
    "print(flood_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m stations \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./gauges.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m flood_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./floods.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfind_mf_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstations\u001b[49m\u001b[43m,\u001b[49m\u001b[43mflood_data\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[18], line 29\u001b[0m, in \u001b[0;36mfind_mf_func\u001b[1;34m(stations_tab, amax_tab, start_year)\u001b[0m\n\u001b[0;32m     27\u001b[0m index_MF \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tt \u001b[38;5;129;01min\u001b[39;00m index_outlier:\n\u001b[1;32m---> 29\u001b[0m     ty \u001b[38;5;241m=\u001b[39m \u001b[43mdummy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     30\u001b[0m     nyears_before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dummy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m][dummy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m ty])\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# 3. after start_year and with at least 20 years of data before event\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\indexing.py:1146\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m-> 1146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\frame.py:4012\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   4006\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[0;32m   4008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   4009\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   4010\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   4011\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[1;32m-> 4012\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[row]\n\u001b[0;32m   4015\u001b[0m \u001b[38;5;66;03m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[0;32m   4016\u001b[0m \u001b[38;5;66;03m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda\\envs\\statistical\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "# 有问题\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy.stats import genextreme\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "def find_mf_func(stations_tab, amax_tab, start_year):\n",
    "    megafloods_df = pd.DataFrame(columns=[\"id\", \"LAT\", \"LON\", \"area\", \"Q_MF\", \"year_MF\", \"date_MF\"])\n",
    "    \n",
    "    nn = 0  # count number of outliers\n",
    "    for i in range(stations_tab.shape[0]):\n",
    "        dummy = amax_tab[amax_tab[\"id\"] == stations_tab.loc[i, \"id\"]]\n",
    "        dummy = dummy[~np.isnan(dummy[\"q\"])]\n",
    "        if len(dummy[\"q\"]) < 10:\n",
    "            continue  # exclude short series\n",
    "        \n",
    "        # megafloods criteria:\n",
    "        \n",
    "        # 1. outlier\n",
    "        dummy_quant = np.percentile(dummy[\"q\"], q=[25, 75])\n",
    "        dummy_upper = dummy_quant[1] + 3 * (dummy_quant[1] - dummy_quant[0])\n",
    "        index_outlier = np.where(dummy[\"q\"] >= dummy_upper)[0]\n",
    "        \n",
    "        if len(index_outlier) == 0:\n",
    "            continue\n",
    "        \n",
    "        index_MF = []\n",
    "        for tt in index_outlier:\n",
    "            ty = dummy.loc[tt, \"year\"]\n",
    "            nyears_before = len(dummy[\"q\"][dummy[\"year\"] < ty])\n",
    "            \n",
    "            # 3. after start_year and with at least 20 years of data before event\n",
    "            if nyears_before < 20:\n",
    "                continue\n",
    "            if ty < start_year:\n",
    "                continue\n",
    "            \n",
    "            # 2. surprising (Tl/Tsl>3) record-breaking event\n",
    "            qmax_old = dummy[\"q\"][dummy[\"year\"] <= ty]\n",
    "            shape, loc, scale = genextreme.fit(qmax_old)\n",
    "            Tl = np.max(qmax_old)\n",
    "            Tsl = genextreme.ppf(1 - 1 / len(qmax_old), shape, loc, scale)\n",
    "            rRP = Tl / Tsl\n",
    "            if rRP > 3:\n",
    "                index_MF.append(tt)\n",
    "        \n",
    "        index_outlier = index_MF\n",
    "        \n",
    "        if len(index_outlier) > 0:\n",
    "            for oo in index_outlier:\n",
    "                nn += 1\n",
    "                megafloods_df.loc[nn, \"id\"] = stations_tab.loc[i, \"id\"]\n",
    "                megafloods_df.loc[nn, \"LAT\"] = stations_tab.loc[i, \"lat\"]\n",
    "                megafloods_df.loc[nn, \"LON\"] = stations_tab.loc[i, \"lon\"]\n",
    "                megafloods_df.loc[nn, \"area\"] = float(stations_tab.loc[i, \"area\"])\n",
    "                megafloods_df.loc[nn, \"Q_MF\"] = dummy.loc[oo, \"q\"]  # outlier discharge\n",
    "                megafloods_df.loc[nn, \"year_MF\"] = dummy.loc[oo, \"year\"]  # outlier year\n",
    "                megafloods_df.loc[nn, \"date_MF\"] = datetime.strptime(\n",
    "                    f\"{dummy.loc[oo, 'year']}-{dummy.loc[oo, 'month']}-{dummy.loc[oo, 'day']}\",\n",
    "                    \"%Y-%m-%d\"\n",
    "                )  # outlier date\n",
    "    \n",
    "    return megafloods_df\n",
    "\n",
    "\n",
    "# 完成数据集的建立\n",
    "stations = pd.read_csv(\"./gauges.csv\")\n",
    "flood_data = pd.read_csv(\"./floods.csv\")\n",
    "print(find_mf_func(stations,flood_data,2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一部分：筛选数据\n",
    "问题：统计计算的方法大概率是有问题的，可能需要自己写\n",
    "第二部分：各类计算\n",
    "问题：待定\n",
    "第三部分：预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import genextreme\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "\n",
    "def find_mf_func(stations_tab, amax_tab, start_year):\n",
    "    megafloods_df = pd.DataFrame(columns=['id', 'LAT', 'LON', 'area', 'Q_MF', 'year_MF', 'date_MF'])\n",
    "  \n",
    "    nn = 0  # count number of outliers\n",
    "    for i in range(len(stations_tab)):\n",
    "        station_id = stations_tab['id'][i]\n",
    "        dummy = amax_tab[amax_tab['id'] == station_id].dropna(subset=['q'])\n",
    "        # dummy = amax_tab[amax_tab['id'] == station_id]\n",
    "        \n",
    "        if len(dummy) < 10:\n",
    "            continue  # exclude short series\n",
    "        \n",
    "        # megafloods criteria:\n",
    "        \n",
    "        # 1. outlier \n",
    "        \n",
    "        # dummy_quant = np.percentile(dummy['q'],[25,75],method='')\n",
    "        dummy_quant = np.percentile(dummy['q'],[25,75],method=\"linear\")   # 我去，这个值有点意思 --- 可以细细琢磨下\n",
    "        # dummy_quant = np.percentile(dummy['q'],q = [25,75],method='linear')\n",
    "        dummy_upper = dummy_quant[1] + 3 * (dummy_quant[1] - dummy_quant[0])\n",
    "        # dummy_quant = dummy['q'].quantile([0.25, 0.75])\n",
    "        # dummy_upper = float(dummy_quant[0.75]) + 3 * float((dummy_quant[0.75]) - float(dummy_quant[0.25]))\n",
    "        index_outlier = dummy[dummy['q'] >= dummy_upper].index\n",
    "        if len(index_outlier) == 0:\n",
    "            continue\n",
    "        \n",
    "        index_MF = []\n",
    "        for tt in index_outlier:\n",
    "            ty = dummy['year'][tt]\n",
    "            nyears_before = len(dummy[dummy['year'] < ty])\n",
    "            \n",
    "            # 3. after start.year and with at least 20 years of data before event\n",
    "            if nyears_before < 20 or ty < start_year:\n",
    "                continue\n",
    "            \n",
    "            # 2. surprising (Tl/Tsl > 3) record-breaking event\n",
    "            qmax_old = dummy[dummy['year'] <= ty]['q']\n",
    "            local_gev = genextreme.fit(qmax_old)\n",
    "            neP = genextreme.cdf(qmax_old, *local_gev)\n",
    "            RP = 1 / (1 - neP)\n",
    "            RP[RP == float('inf')] = 10 ** 5\n",
    "            rRP = RP[-1] / max(RP[:-1])\n",
    "            if rRP > 3:\n",
    "                index_MF.append(tt)\n",
    "                \n",
    "        index_outlier = index_MF\n",
    "        if len(index_MF) > 0:\n",
    "            for oo in index_MF:\n",
    "                nn += 1\n",
    "                try:\n",
    "                    megafloods_df.loc[nn] = [stations_tab['id'][i], stations_tab['lat'][i], stations_tab['lon'][i],\n",
    "                                         float(stations_tab['area'][i]), dummy['q'][oo], dummy['year'][oo],\n",
    "                                         pd.to_datetime(\n",
    "                                             f\"{int(dummy['year'][oo])}-{int(dummy['month'][oo])}-{int(dummy['day'][oo])}\")]\n",
    "                except ValueError:\n",
    "                    megafloods_df.loc[nn] = [stations_tab['id'][i], stations_tab['lat'][i], stations_tab['lon'][i],\n",
    "                                         float(stations_tab['area'][i]), dummy['q'][oo], dummy['year'][oo],-1]\n",
    "    \n",
    "    return megafloods_df\n",
    "\n",
    "stations = pd.read_csv(\"./gauges.csv\")\n",
    "flood_data = pd.read_csv(\"./floods.csv\")\n",
    "megafloods_df = find_mf_func(stations,flood_data,2000)\n",
    "# print(find_mf_func(stations,flood_data,2000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二部分\n",
    "import pandas as pd\n",
    "\n",
    "stations['mean_q'] = pd.Series([None] * len(stations))\n",
    "stations['max_q'] = pd.Series([None] * len(stations))\n",
    "\n",
    "for i in range(len(stations)):\n",
    "    qq = flood_data[flood_data['id'] == stations['id'][i]][2]\n",
    "    stations.at[i, 'mean_q'] = round(qq.mean(), 5)\n",
    "    stations.at[i, 'max_q'] = qq.max()\n",
    "    # del qq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mstats\n",
    "\n",
    "def envelope_curve(stations_tab, amax_tab):\n",
    "    ENV_tab = pd.DataFrame(columns=[\"slope_qr_median\", \"intercept_qr_median\", \"slope_qr_env\", \"intercept_qr_env\", \"intercept_env\", \"n_stat_years\"])\n",
    "    \n",
    "    for rr in range(1, 6):\n",
    "        print(rr)\n",
    "        dummy_stations = stations_tab[stations_tab[\"regions1\"] == rr]\n",
    "        dummy_q = amax_tab[amax_tab[\"id\"].isin(dummy_stations[\"id\"]) & (amax_tab[\"q\"] > 0)]\n",
    "        dummy_q[\"qlog10\"] = np.log10(dummy_q[\"q\"])\n",
    "        dummy_q[\"Alog10\"] = np.nan\n",
    "        \n",
    "        for i in range(len(dummy_stations)):\n",
    "            ind_stat = dummy_q.index[dummy_q[\"id\"] == dummy_stations[\"id\"][i]]\n",
    "            dummy_q.at[ind_stat, \"Alog10\"] = np.log10(dummy_stations[\"area\"][i])\n",
    "        \n",
    "        # quantile regression z=0.5\n",
    "        p = 1\n",
    "        dummy_rq = mstats.quantreg(dummy_q[\"qlog10\"], dummy_q[\"Alog10\"], alpha=0.5)\n",
    "        ENV_tab[ \"slope_qr_median\"][rr-1] = dummy_rq.params[1] # slope of qr\n",
    "        ENV_tab[\"intercept_qr_median\"][rr-1] = dummy_rq.params[0] # intercept of qr\n",
    "        \n",
    "        # quantile regression z=0.999\n",
    "        dummy_rq = mstats.quantreg(dummy_q[\"qlog10\"], dummy_q[\"Alog10\"], alpha=0.999)\n",
    "        ENV_tab[ \"slope_qr_env\"][rr-1]  = dummy_rq.params[1] # slope of qr\n",
    "        ENV_tab[\"intercept_qr_env\"][rr-1]  = dummy_rq.params[0] # intercept of qr\n",
    "        \n",
    "        intercept = -ENV_tab[\"slope_qr_env\"][rr-1] * np.log10(dummy_stations[\"area\"]) + np.log10(dummy_stations[\"max_q\"])\n",
    "        ENV_tab[\"intercept_env\"][rr-1]  = max(intercept) # intercept of envelope\n",
    "        ENV_tab[\"n_stat_years\"][rr-1]  = len(dummy_q) # nstatyears\n",
    "    \n",
    "    return ENV_tab\n",
    "\n",
    "ENV_reg = envelope_curve(stations,flood_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# EDT 1\n",
    "def ext_data_tab1_func(envelope_tab):\n",
    "    ext_data_table1 = pd.DataFrame(data=None, columns=[\"Slope_Envelope\", \"Intercept_Envelope\", \"Intercept_Envelope_1000km2\", \"Slope_median_regression\", \"Intercept_median_regression\"], index=envelope_tab.index)\n",
    "    ext_data_table1[\"Slope_Envelope\"] = envelope_tab[\"slope_qr_env\"]\n",
    "    ext_data_table1[\"Intercept_Envelope\"] = envelope_tab[\"intercept_env\"]\n",
    "    ext_data_table1[\"Intercept_Envelope_1000km2\"] = 10**(envelope_tab[\"intercept_env\"] + envelope_tab[\"slope_qr_env\"] * (np.log10(1000)))\n",
    "    ext_data_table1[\"Slope_median_regression\"] = envelope_tab[\"slope_qr_median\"]\n",
    "    ext_data_table1[\"Intercept_median_regression\"] = envelope_tab[\"intercept_qr_median\"]\n",
    "    ext_data_table1 = round(ext_data_table1, 2)\n",
    "    return ext_data_table1\n",
    "\n",
    "ExtDataTable1 = ext_data_tab1_func(ENV_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# EDT 2\n",
    "ExtDataTable2 = pd.DataFrame(columns=[\"Number_of_gauges\", \"Avg_Ratio_max_mean_Q\", \"Number_of_targets\", \"Percentage_of_targets\", \"Avg_Ratio_max_mean_Q_targets\"])\n",
    "\n",
    "for rr in range(1, 6):\n",
    "    dummy_stations = stations[stations[\"regions1\"] == rr]\n",
    "    ExtDataTable2.at[rr-1, \"Number_of_gauges\"] = len(dummy_stations)\n",
    "    ExtDataTable2.at[rr-1, \"Avg_Ratio_max_mean_Q\"] = round(np.mean(dummy_stations[\"max_q\"] / dummy_stations[\"mean_q\"]), 2)\n",
    "    dummy_mf = dummy_stations[dummy_stations[\"id\"].isin(megafloods_df[\"id\"])]\n",
    "    ExtDataTable2.at[rr-1, \"Number_of_targets\"] = len(dummy_mf)\n",
    "    ExtDataTable2.at[rr-1, \"Percentage_of_targets\"] = round(ExtDataTable2.at[rr-1, \"Number_of_targets\"] * 100 / ExtDataTable2.at[rr-1, \"Number_of_gauges\"], 1)\n",
    "    ExtDataTable2.at[rr-1, \"Avg_Ratio_max_mean_Q_targets\"] = round(np.mean(dummy_mf[\"max_q\"] / dummy_mf[\"mean_q\"]), 2)\n",
    "\n",
    "ExtDataTable2.index = ExtDataTable2.index + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import iqr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Fig. 2 & Fig. 4\n",
    "\n",
    "# mean and cv up to target years\n",
    "def mhq_cv_func(stations_tab, amax_tab, target_years):\n",
    "    EU_MHQ_CV_list = {}\n",
    "    for YY in range(len(target_years)):\n",
    "        print(target_years[YY])\n",
    "        TARGET_YEAR = target_years[YY]\n",
    "        EU_MHQ_CV = pd.DataFrame(np.nan, index=range(stations_tab.shape[0]), columns=[\"id\", \"lat\", \"lon\", \"area\", \"MHQ\", \"CV\", \"MHQ_100\"])\n",
    "        EU_MHQ_CV[[\"id\", \"lat\", \"lon\", \"area\"]] = stations_tab[[\"id\", \"lat\", \"lon\", \"area\"]]\n",
    "        for rrr in range(EU_MHQ_CV.shape[0]):\n",
    "            dummy_tab = amax_tab[(amax_tab[\"id\"]==EU_MHQ_CV.iloc[rrr, 0])&(amax_tab[\"year\"]<TARGET_YEAR)].dropna(subset=[\"q\"])\n",
    "            if dummy_tab.shape[0] < 10:\n",
    "                continue\n",
    "            dummy_Q = dummy_tab[\"q\"].values\n",
    "            dummy_Y = dummy_tab[\"year\"].values\n",
    "            dummy_quant = np.quantile(dummy_Q, q=[0.25, 0.75])\n",
    "            dummy_upper = dummy_quant[1] + 3*(dummy_quant[1] - dummy_quant[0])\n",
    "            dummy_Y = dummy_Y[dummy_Q <= dummy_upper]\n",
    "            dummy_Q = dummy_Q[dummy_Q <= dummy_upper]\n",
    "            EU_MHQ_CV.iloc[rrr, 4] = np.mean(dummy_Q)\n",
    "            EU_MHQ_CV.iloc[rrr, 5] = iqr(dummy_Q)/EU_MHQ_CV.iloc[rrr, 4]\n",
    "        # normalize MHQ to 100km2\n",
    "        area_regr = LinearRegression().fit(np.log10(EU_MHQ_CV[[\"area\"]]), np.log10(EU_MHQ_CV[[\"MHQ\"]]))\n",
    "        EU_MHQ_CV.iloc[:, 6] = 10**(np.log10(EU_MHQ_CV[\"MHQ\"]) - area_regr.coef_[0]*np.log10(EU_MHQ_CV[\"area\"]) + area_regr.coef_[0]*np.log10(100))\n",
    "        EU_MHQ_CV_list[target_years[YY]] = EU_MHQ_CV\n",
    "    return EU_MHQ_CV_list\n",
    "\n",
    "EU_MHQ_CV_list = mhq_cv_func(stations, flood_data, list(range(2000, 2023)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第三部分\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def predict_mf_function(mf_list, stations_tab, target_years, amax_tab, mhq_cv_tab, env_tab):\n",
    "    # specify weights and distance limit for finding donor group\n",
    "    w1 = 1\n",
    "    w2 = 1\n",
    "    w3 = 1\n",
    "    DIST = 1\n",
    "\n",
    "    # prepare lists for saving results\n",
    "    REG_MEGAFLOODS_list = [None] * 5  # List megafloods per each region\n",
    "    REG_ENVELOPE_param = [None] * 5  # Envelope parameters\n",
    "    REG_DONORS_list = [None] * 5  # Donor list per region per megaflood\n",
    "\n",
    "    for REG in range(1, 6):  # loop on Regions\n",
    "        REG_list_stations = stations_tab['id'][stations_tab['regions1'] == REG]\n",
    "        MEGAFLOODS_list = mf_list[mf_list['id'].isin(REG_list_stations)]\n",
    "        DONORS_list = [None] * MEGAFLOODS_list.shape[0]\n",
    "        ENVELOPE_param = pd.DataFrame(np.nan, index=range(MEGAFLOODS_list.shape[0]), columns=[\"slope\", \"intercept.region\", \"intercept.mf\", \"q.predicted\", \"n_statyears_donors\", \"n_stat_donors\"])\n",
    "\n",
    "        for i in range(MEGAFLOODS_list.shape[0]):  # loop on megafloods\n",
    "            print(i)\n",
    "\n",
    "            TARGET_YEAR = MEGAFLOODS_list['year_MF'].iloc[i]\n",
    "            TARGET_Q = MEGAFLOODS_list['Q_MF'].iloc[i]\n",
    "            TARGET_DATE = MEGAFLOODS_list['date_MF'].iloc[i]\n",
    "            code_mf = MEGAFLOODS_list['id'].iloc[i]\n",
    "            EU_MHQ_CV = mhq_cv_tab[target_years.index(TARGET_YEAR)]\n",
    "            EU_MHQ_CV = EU_MHQ_CV.dropna(subset=['MHQ'])\n",
    "            EU_MHQ_CV = EU_MHQ_CV[EU_MHQ_CV['id'].isin(REG_list_stations)]\n",
    "\n",
    "            # distances\n",
    "            ind_stat = EU_MHQ_CV[EU_MHQ_CV['id'] == code_mf].index\n",
    "            sd1 = (np.log10(EU_MHQ_CV['area']) - np.mean(np.log10(EU_MHQ_CV['area']))) / np.std(np.log10(EU_MHQ_CV['area']))\n",
    "            sd2 = (np.log10(EU_MHQ_CV['MHQ_100']) - np.mean(np.log10(EU_MHQ_CV['MHQ_100']))) / np.std(np.log10(EU_MHQ_CV['MHQ_100']))\n",
    "            sd3 = (np.log10(EU_MHQ_CV['CV']) - np.mean(np.log10(EU_MHQ_CV['CV']))) / np.std(np.log10(EU_MHQ_CV['CV']))\n",
    "            d1 = sd1 - sd1[ind_stat].values[0]\n",
    "            d2 = sd2 - sd2[ind_stat].values[0]\n",
    "            d3 = sd3 - sd3[ind_stat].values[0]\n",
    "            dd = np.sqrt((d1**2) * w1 + (d2**2) * w2 + (d3**2) * w3)\n",
    "            # donor group\n",
    "            group = EU_MHQ_CV.loc[dd <= DIST, 'id']\n",
    "            stations_Reg_group = stations_tab[stations_tab['id'].isin(group)]\n",
    "            EU_MHQ_CV_group = EU_MHQ_CV[EU_MHQ_CV['id'].isin(group)]\n",
    "            # characteristics of donor catchments\n",
    "            DONORS = pd.DataFrame(columns=[\"id\", \"lat\", \"lon\", \"area\", \"SFOR.B\", \"SFOR.year.B\", \"SFOR.date.B\"])\n",
    "            for s in range(EU_MHQ_CV_group.shape[0]):\n",
    "                DONORS.loc[s, \"id\"] = int(EU_MHQ_CV_group.iloc[s]['id'])\n",
    "                DONORS.loc[s, \"lat\"] = float(EU_MHQ_CV_group.iloc[s]['lat'])\n",
    "                DONORS.loc[s, \"lon\"] = float(EU_MHQ_CV_group.iloc[s]['lon'])\n",
    "                DONORS.loc[s, \"area\"] = float(EU_MHQ_CV_group.iloc[s]['area'])\n",
    "                dummy = amax_tab[amax_tab['id'] == EU_MHQ_CV_group.iloc[s]['id']]\n",
    "                QQ = dummy[dummy['year'] < TARGET_YEAR]['q'].values\n",
    "                DD = pd.to_datetime(dummy[dummy['year'] < TARGET_YEAR][['year', 'month', 'day']].astype(str).agg('-'.join, axis=1))\n",
    "                YY = dummy[dummy['year'] < TARGET_YEAR]['year'].values\n",
    "                ind_SFOR = np.argmax(QQ)\n",
    "                if len(ind_SFOR) > 1:\n",
    "                    ind_SFOR = np.max(ind_SFOR)\n",
    "                DONORS.loc[s, \"SFOR.B\"] = QQ[ind_SFOR]\n",
    "                DONORS.loc[s, \"SFOR.year.B\"] = YY[ind_SFOR]\n",
    "                DONORS.loc[s, \"SFOR.date.B\"] = DD[ind_SFOR]\n",
    "                dummy, ind_SFOR, QQ, YY, DD = None, None, None, None, None\n",
    "\n",
    "            # envelope with region slope\n",
    "            ENVELOPE_param.loc[i, \"slope\"] = env_tab['slope_qr_env'][REG - 1]\n",
    "            ENVELOPE_param.loc[i, \"intercept.region\"] = env_tab['intercept_qr_env'][REG - 1]\n",
    "            intercept = -ENVELOPE_param.loc[i, \"slope\"] * np.log10(DONORS['area']) + np.log10(DONORS['SFOR.B'])\n",
    "            DONORS['intercept'] = intercept\n",
    "            DONORS = DONORS.sort_values(by=\"intercept\", ascending=False)\n",
    "            DONORS_list[i] = DONORS\n",
    "            ENVELOPE_param.loc[i, \"intercept.mf\"] = np.max(intercept)\n",
    "            ENVELOPE_param.loc[i, \"q.predicted\"] = ENVELOPE_param.loc[i, \"intercept.mf\"] + ENVELOPE_param.loc[i, \"slope\"] * np.log10(MEGAFLOODS_list['area'].iloc[i])\n",
    "            ENVELOPE_param.loc[i, \"n_statyears_donors\"] = amax_tab[amax_tab['id'].isin(DONORS['id'])].shape[0]  # n station-years in donor group\n",
    "            ENVELOPE_param.loc[i, \"n_stat_donors\"] = len(group)\n",
    "\n",
    "        REG_MEGAFLOODS_list[REG - 1] = MEGAFLOODS_list\n",
    "        REG_ENVELOPE_param[REG - 1] = ENVELOPE_param\n",
    "        REG_DONORS_list[REG - 1] = DONORS_list\n",
    "\n",
    "    REG_MEGAFLOODS_list = dict(zip(range(1, 6), REG_MEGAFLOODS_list))\n",
    "    REG_ENVELOPE_param = dict(zip(range(1, 6), REG_ENVELOPE_param))\n",
    "    REG_DONORS_list = dict(zip(range(1, 6), REG_DONORS_list))\n",
    "\n",
    "    RES = {\"mf.list\": REG_MEGAFLOODS_list, \"EC.param\": REG_ENVELOPE_param, \"donor.list\": REG_DONORS_list}\n",
    "    return RES\n",
    "\n",
    "MF_predicted = predict_mf_function(megafloods_df, stations, list(range(2000, 2023)), flood_data, EU_MHQ_CV_list, ENV_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from math import atan2, pi\n",
    "\n",
    "def PREDvsOBS_plot(mf_pred_list):\n",
    "    # prepare lists for saving results\n",
    "    id_mf = []\n",
    "    lat = []\n",
    "    lon = []\n",
    "    region = []\n",
    "    q_obs = []\n",
    "    q_pred = []\n",
    "    ratio = []\n",
    "    doy_target = []\n",
    "    doy_donors = []\n",
    "    r_donors = []\n",
    "\n",
    "    for rr in range(1, 6):\n",
    "        ENVELOPE_param = mf_pred_list[\"EC.param\"][rr-1]\n",
    "        MEGAFLOODS_list = mf_pred_list[\"mf.list\"][rr-1]\n",
    "        DONORS_list = mf_pred_list[\"donor.list\"][rr-1]\n",
    "\n",
    "        id_mf_r = MEGAFLOODS_list['id'].tolist()\n",
    "        lat_r = MEGAFLOODS_list['LAT'].tolist()\n",
    "        lon_r = MEGAFLOODS_list['LON'].tolist()\n",
    "        region_r = [rr] * len(id_mf_r)\n",
    "\n",
    "        # q\n",
    "        q_obs_r = MEGAFLOODS_list['Q_MF'].tolist()\n",
    "        q_pred_r = np.round(10**ENVELOPE_param['q.predicted'], 5).tolist()\n",
    "\n",
    "        # timing\n",
    "        m = 365.25\n",
    "        doy_target_r = [(datetime.strptime(date_str, '%Y-%m-%d').timetuple().tm_yday) * 2 * pi / m for date_str in MEGAFLOODS_list['date_MF']]\n",
    "        teta_donors_r = []\n",
    "        doy_donors_r = []\n",
    "        r_donors_r = []\n",
    "        for mm in range(len(DONORS_list)):\n",
    "            # 10 donors\n",
    "            DONORS = DONORS_list[mm]\n",
    "            uu = min(10, DONORS.shape[0])\n",
    "            DONORS = DONORS.iloc[:uu, :]\n",
    "            doy = [(datetime.strptime(date_str, '%Y-%m-%d').timetuple().tm_yday) * 2 * pi / m for date_str in DONORS['SFOR.date.B']]\n",
    "            x = [np.cos(t) for t in doy]\n",
    "            y = [np.sin(t) for t in doy]\n",
    "            x_donor = np.mean(x)\n",
    "            y_donor = np.mean(y)\n",
    "            teta_donors_r.append(atan2(y_donor, x_donor))\n",
    "            doy_donors_r.append(round(teta_donors_r[mm]*m/(2*pi), 0))\n",
    "            if doy_donors_r[mm] < 0:\n",
    "                doy_donors_r[mm] += m\n",
    "            r_donors_r.append(round((x_donor**2 + y_donor**2)**0.5, 2))\n",
    "\n",
    "        # append to lists\n",
    "        id_mf.extend(id_mf_r)\n",
    "        lat.extend(lat_r)\n",
    "        lon.extend(lon_r)\n",
    "        region.extend(region_r)\n",
    "        q_obs.extend(q_obs_r)\n",
    "        q_pred.extend(q_pred_r)\n",
    "        doy_target.extend(doy_target_r)\n",
    "        doy_donors.extend(doy_donors_r)\n",
    "        r_donors.extend(r_donors_r)\n",
    "\n",
    "    ratio = np.round(np.array(q_obs) / np.array(q_pred), 2).tolist()\n",
    "    PREDvsOBS_mf = pd.DataFrame({\n",
    "        'id.mf': id_mf,\n",
    "        'lat': lat,\n",
    "        'lon': lon,\n",
    "        'region': region,\n",
    "        'Q.obs': q_obs,\n",
    "        'Q.pred': q_pred,\n",
    "        'Ratio': ratio,\n",
    "        'doy.target': doy_target,\n",
    "        'doy.donors': doy_donors,\n",
    "        'R.donors': r_donors\n",
    "    })\n",
    "    return PREDvsOBS_mf\n",
    "\n",
    "PREDvsOBS_mf = PREDvsOBS_plot(MF_predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statistical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
